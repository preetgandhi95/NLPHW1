{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by downloading 20-newsgroup text dataset:\n",
    "\n",
    "```http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=!ls aclImdb/train/pos\n",
    "train_neg=!ls aclImdb/train/neg\n",
    "test_pos=!ls aclImdb/test/pos\n",
    "test_neg=!ls aclImdb/test/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_targets=[]\n",
    "for i in range(0,len(train_pos)):\n",
    "    with open (\"aclImdb/train/pos/\"+train_pos[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(1))\n",
    "for i in range(0,len(train_neg)):\n",
    "    with open (\"aclImdb/train/neg/\"+train_neg[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_targets=[]\n",
    "for i in range(0,len(test_pos)):\n",
    "    with open (\"aclImdb/test/pos/\"+test_pos[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(1))\n",
    "for i in range(0,len(test_neg)):\n",
    "    with open (\"aclImdb/test/neg/\"+test_neg[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data[10000:12500] + train_data[22500:25000]\n",
    "val_targets=train_targets[10000:12500] + train_targets[22500:25000]\n",
    "\n",
    "train_data = train_data[0:10000] + train_data[12500:22500]\n",
    "train_targets =  train_targets[0:10000] + train_targets[12500:22500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_data.copy()\n",
    "y=test_data.copy()\n",
    "z=val_data.copy()\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "val_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(x)):\n",
    "    train_data.append(x[i][0])\n",
    "for i in range(0,len(y)):\n",
    "    test_data.append(y[i][0])\n",
    "for i in range(0,len(z)):\n",
    "    val_data.append(z[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords={'\\n','\\t','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', \n",
    "           'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an',\n",
    "           'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself',\n",
    "           'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "           'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n",
    "           'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "           'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all',\n",
    "           'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "           'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', \n",
    "           'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "           'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few',\n",
    "           'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it',\n",
    "           'how', 'further', 'was', 'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'looking', 'buying', 'u.k.', 'startup', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    ty= [token for token in u if (token not in stopwords)]\n",
    "    return ty\n",
    "\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking', 'looking buying', 'buying u.k.', 'u.k. startup', 'startup 1', '1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize2(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-1):\n",
    "        t.append(temp[i]+ ' '+temp[i+1])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize2(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying', 'looking buying u.k.', 'buying u.k. startup', 'u.k. startup 1', 'startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize3(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-2):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize3(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying u.k.', 'looking buying u.k. startup', 'buying u.k. startup 1', 'u.k. startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize4(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-3):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2]+' '+temp[i+3])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize4(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "#Function to tokenize food dataset. \n",
    "#Goes through every doc in dataset and converts to tokens.  Takes 15-20 minutes\n",
    "#Split documents in parallel and then tokenize.\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    token_dataset2 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens2 = []    \n",
    "    token_dataset3 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens3 = []    \n",
    "    token_dataset4 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens4 = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        tokens2 = tokenize2(sample)\n",
    "        tokens3 = tokenize3(sample)\n",
    "        tokens4 = tokenize4(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        token_dataset2.append(tokens2)\n",
    "        all_tokens2 += tokens2\n",
    "        token_dataset3.append(tokens3)\n",
    "        all_tokens3 += tokens3\n",
    "        token_dataset4.append(tokens4)\n",
    "        all_tokens4 += tokens4\n",
    "    return token_dataset, all_tokens,token_dataset2, all_tokens2,token_dataset3, all_tokens3,token_dataset4, all_tokens4\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5181552\n"
     ]
    }
   ],
   "source": [
    "#For bi-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"nntest_bi_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nnval_bi_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"nntrain_bi_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"nntrain_bi_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 6908 ; token though movie\n",
      "Token though movie; token id 6908\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 77.72\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.28\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 86.2\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.44\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 87.06\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.86\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.46\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.34\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 86.14\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.44\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 86.66\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 86.18\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 85.78\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.9\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 85.58\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 83.98\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 85.28\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.52\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 84.8\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.7\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.04\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 83.92\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.32\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.44\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 83.92\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.16\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 83.92\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.32\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 83.44\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 83.68\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 82.92\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.4\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.1\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 82.94\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.42\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 82.02\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 83.22\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 83.0\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 83.36\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 82.9\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.26\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 83.18\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.32\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 82.86\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.98\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.92\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 83.16\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 82.94\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 83.08\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.94\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 83.12\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 83.0\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.54\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 83.02\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 83.12\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.96\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.84\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 82.84\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.32\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.74\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa1=pa\n",
    "pl1=pl\n",
    "epa1=epa\n",
    "epl1=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 82.18\n",
      "Test Acc 82.604\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 8018 ; token playboy\n",
      "Token playboy; token id 8018\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 68.08\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.62\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 80.8\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.88\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 86.46\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.04\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.04\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.22\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 86.22\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.7\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 85.12\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 84.52\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.86\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.42\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 84.54\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 84.84\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 82.76\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.64\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 84.36\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.3\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 83.56\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.3\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.36\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.14\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 83.9\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 83.9\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 83.68\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 83.96\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.06\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.18\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.94\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 84.06\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 84.16\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 84.04\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.98\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 84.18\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 84.06\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 84.16\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 84.04\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 83.84\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.9\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 84.02\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.98\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 84.0\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 83.92\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 83.98\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 83.94\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 83.96\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 83.92\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 84.04\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 84.06\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 84.02\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 83.86\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 83.9\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 83.9\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 83.88\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 83.88\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 83.82\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 83.9\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 83.98\n",
      "After training for 10 epochs\n",
      "Val Acc 83.8\n",
      "Test Acc 85.232\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 30000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa2=pa\n",
    "pl2=pl\n",
    "epa2=epa\n",
    "epl2=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 36750 ; token barrels\n",
      "Token barrels; token id 36750\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 76.32\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.94\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 85.26\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.14\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 83.18\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 85.38\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 85.08\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 84.2\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.4\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 84.78\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 84.98\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.84\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.58\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 84.42\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 84.3\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 84.76\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 84.3\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 83.78\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.06\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 83.78\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.18\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.62\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.24\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.08\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.22\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.02\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.2\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.2\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 84.22\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 84.24\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 84.1\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 84.46\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 84.36\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 84.4\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 84.18\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 84.26\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 84.16\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 84.26\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 84.32\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 84.18\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 84.3\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 84.32\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 84.18\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 84.26\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 84.16\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 84.22\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 84.16\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 84.22\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 84.16\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 84.12\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 84.08\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 84.04\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 84.22\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 84.08\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 84.12\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 84.1\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 84.12\n",
      "After training for 10 epochs\n",
      "Val Acc 84.06\n",
      "Test Acc 85.956\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 50000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa3=pa\n",
    "pl3=pl\n",
    "epa3=epa\n",
    "epl3=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 58803 ; token miyoshi\n",
      "Token miyoshi; token id 58803\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 76.78\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.88\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 85.02\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.48\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 84.94\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 87.1\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.3\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.04\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 86.24\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.76\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 85.34\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 83.58\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.66\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.84\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 84.16\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 84.22\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 84.84\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 84.3\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 84.14\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 83.56\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 83.96\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.08\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 83.68\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.2\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.22\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 83.86\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.2\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.02\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.08\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.46\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.92\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 84.36\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 84.34\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 84.36\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 84.24\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 83.9\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 84.08\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 84.16\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 84.14\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 84.18\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 84.12\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.94\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 83.94\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 84.12\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 84.22\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 84.02\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 84.06\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 84.22\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 84.06\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 84.22\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 84.24\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 84.1\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 84.2\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 84.26\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 84.08\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 84.02\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 84.0\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 84.06\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 84.08\n",
      "After training for 10 epochs\n",
      "Val Acc 83.92\n",
      "Test Acc 85.964\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 70000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa4=pa\n",
    "pl4=pl\n",
    "epa4=epa\n",
    "epl4=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 89805 ; token 's angel\n",
      "Token 's angel; token id 89805\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 76.54\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.74\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 85.72\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 82.82\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 85.16\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 85.76\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 85.64\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 85.54\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 85.42\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.84\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 84.62\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 84.08\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.38\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.44\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 84.46\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 84.3\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 84.5\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.28\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 84.04\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.26\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.4\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.3\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.18\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.04\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.12\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.1\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.04\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.1\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.2\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 84.22\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 84.14\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 84.24\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 84.16\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.94\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 84.1\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 84.16\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 84.14\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 84.2\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 84.24\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 84.1\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 84.08\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 84.24\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 84.04\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 84.22\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 84.08\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 84.1\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 84.14\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 84.06\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 84.18\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 84.14\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 84.08\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 84.0\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 84.06\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 84.2\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 84.1\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 84.08\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 83.98\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 84.04\n",
      "After training for 10 epochs\n",
      "Val Acc 84.14\n",
      "Test Acc 85.984\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 100000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa5=pa\n",
    "pl5=pl\n",
    "epa5=epa\n",
    "epl5=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 130960 ; token production since\n",
      "Token production since; token id 130960\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 73.3\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 85.46\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 86.26\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 85.06\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 86.9\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 85.92\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 85.22\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 84.38\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 84.26\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.0\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 84.68\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 82.96\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.4\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.34\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 83.84\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 83.88\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 84.04\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 84.22\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 83.56\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 83.92\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 83.86\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 83.96\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.06\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.06\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 83.82\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.12\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.06\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 83.84\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 83.94\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.02\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.98\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.98\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.92\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 84.02\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.86\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.82\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 83.9\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 83.86\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 83.8\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 83.92\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.72\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 83.88\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.88\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 83.9\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 83.88\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 83.9\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 83.84\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 83.82\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 83.86\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 83.9\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 83.76\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 83.9\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 83.94\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 83.86\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 83.8\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 83.74\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 83.84\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 83.8\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 83.8\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 83.78\n",
      "After training for 10 epochs\n",
      "Val Acc 83.72\n",
      "Test Acc 86.096\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 150000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa6=pa\n",
    "pl6=pl\n",
    "epa6=epa\n",
    "epl6=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 202231 ; token bumbling guy\n",
      "Token bumbling guy; token id 202231\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 72.96\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 80.84\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 80.32\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 84.06\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 85.2\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.16\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 85.0\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 82.86\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 85.34\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 82.86\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 82.18\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 83.24\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 83.16\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 83.24\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 82.24\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 83.38\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 82.64\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 82.86\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 82.62\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 82.64\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 82.54\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 82.72\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.74\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 82.4\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 82.36\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 82.4\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.74\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 82.72\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 82.72\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.58\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 82.72\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 82.44\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 82.74\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 82.56\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 82.74\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 82.36\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 82.36\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 82.78\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 82.56\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 82.66\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 82.62\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 82.56\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 82.66\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 82.6\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.54\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.66\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 82.6\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 82.4\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 82.44\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.6\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 82.6\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.4\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.62\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 82.44\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 82.56\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.42\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.4\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 82.58\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.34\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.68\n",
      "After training for 10 epochs\n",
      "Val Acc 82.58\n",
      "Test Acc 85.596\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 250000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa7=pa\n",
    "pl7=pl\n",
    "epa7=epa\n",
    "epl7=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 194347 ; token pirate named\n",
      "Token pirate named; token id 194347\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Epoch: [1/10], Step: [101/625], Validation Acc: 66.62\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.84\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 83.46\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 85.38\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 83.52\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 83.06\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 82.94\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 83.5\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 84.66\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 80.32\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 79.38\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 83.04\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 82.04\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 81.32\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 82.38\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 81.76\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 82.8\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 82.68\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 82.46\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 82.08\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 82.24\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 82.46\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.06\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 81.94\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 82.34\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 82.0\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.3\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 82.14\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 82.3\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.36\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 82.2\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 82.0\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 82.18\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 82.24\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 82.0\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 82.3\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 82.14\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 82.14\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 82.38\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 82.34\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 82.12\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 82.48\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 82.16\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 82.04\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.3\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.34\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 82.32\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 82.24\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 82.46\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.22\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 82.1\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.2\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.18\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 82.2\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 82.36\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.28\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.1\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 82.26\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.4\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.3\n",
      "After training for 10 epochs\n",
      "Val Acc 82.26\n",
      "Test Acc 85.632\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 500000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break\n",
    "\n",
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "\n",
    "pa8=pa\n",
    "pl8=pl\n",
    "epa8=epa\n",
    "epl8=epl\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzsnXd8VUX6h5+5Pb0nJCT0UEMvAiIgRaogsCq4FtaCq1jWtbEqoqLYUBGxgIKKrvpT6SgsSFNAQECQTjAB0iCQfpPcPr8/zk2DJNwbkgByHj7zOefOmZnz3gvM98y8Z94RUkpUVFRUVK5eNJfaABUVFRWVS4sqBCoqKipXOaoQqKioqFzlqEKgoqKicpWjCoGKiorKVY4qBCoqKipXOaoQqKioqFzlqEKgoqKicpWjCoGKiorKVY7uUhvgCeHh4bJJkyaX2gwVFRWVK4pdu3adlVJGXKjcFSEETZo0YefOnZfaDBUVFZUrCiHECU/KqVNDKioqKlc5qhCoqKioXOWoQqCioqJylXNF+AhUVFRUvMFut5OamorFYrnUptQLJpOJ2NhY9Hp9jeqrQqCiovKXIzU1lYCAAJo0aYIQ4lKbU6dIKcnKyiI1NZWmTZvWqA11akhFReUvh8ViISws7C8vAgBCCMLCwi5q9KMKgYqKyl+Sq0EESrjY76oKQT2w8UgmxzLNl9oMFRUVlUpRhaCOOZiez92f/cbET3dQZHNcanNUVFTqibvvvpvIyEgSEhJK87Kzsxk8eDDx8fEMHjyYnJwcAF544QVmzpx5qUxVhaAukVLy/LL9+Bl0pOYU887ao5faJBUVlXpi4sSJrF69ukLea6+9xsCBA0lMTGTgwIG89tprl8i6iqhCUIcs3p3GzhM5TB3Zlgk9GjF/czL70/IutVkqKir1QN++fQkNDa2Qt2zZMu666y4A7rrrLpYuXXpevY8//phhw4ZRXFxcL3aC+vponZFXbOfVVYfo3CiYv3WNZUhCA346dJqnF/3BssnXotOqGqyiUh+8uOIAB9Pza7XNtjGBTLuxndf1Tp8+TXR0NADR0dFkZmZWuD5nzhzWrFnD0qVLMRqNtWKrJ6i9UR3xztqjZBXamD46AY1GEOSj58VR7TiQns+nW45favNUVFQuM7744gtWrVrFokWL6lUEoI5HBEKIx4B7AQnsA/4BWIGXgZsBJ/ChlHJ2XdpR3xxMz2fhr8e5/ZrGJDQMKs0fltCAQW0ieXvtUYYmNCAu1PfSGamicpVQkyf3uiIqKoqMjAyio6PJyMggMjKy9FpCQgJ79uy5qIVhNaXORgRCiIbAI0A3KWUCoAXGAxOBOKC1lLIN8E1d2XApkFIybfl+gn0NPHFDqwrXhBC8NDoBjYBnl+5HSnmJrFRRUbkUjBo1is8//xyAzz//nNGjR5de69y5M3PnzmXUqFGkp6fXq111PTWkA3yEEDrAF0gHHgBeklK6AKSUmdXUv+JY8nsavx3P4emhrQjyPT/uR0ywD08OacXPR8+wfG/9/mWrqKjUHxMmTKBXr14cOXKE2NhY5s+fz5QpU1i7di3x8fGsXbuWKVOmVKjTp08fZs6cyYgRIzh79my92Srq8qlUCPEo8ApQDKyRUv5dCJEFvA2MAc4Aj0gpEyupOwmYBNCoUaOuJ054tL/CJSXfYmfAzE3Ehviw+IHeaDSVr/ZzuiTjPtxKSnYRP/27HyF+hnq2VEXlr82hQ4do06bNpTajXqnsOwshdkkpu12obl1ODYUAo4GmQAzgJ4S4HTACFrdxHwMLKqsvpZwnpewmpewWEXHBndYuCxQHsbXUQVwVWo3g1bHtySu288qPh+rRQhUVFZXzqcupoUFAspTyjJTSDiwGegOpwCJ3mSVAhzq0od44lJHP51uP8/drGtE+NuiC5dtEBzKpbzO+35XKlmP1NwRUUVFROZe6FIKTQE8hhK9QIiINBA4BS4EB7jL9gCt+uW3JCuIgH/15DuLqeGRgPE3CfHlmyT4sdmcdWqiioqJSNXUmBFLK7cD3wG6UV0c1wDzgNWCcEGIf8CrK66VXNEv3KA7iKcNaE+zr+Xy/Sa9lxpj2nMgq4t1157lJVFRUVOqFOl1HIKWcBkw7J9sKjKjL+9Yn+RY7r/xwmE5xwdzcNc7r+r1bhHNz11jm/ZzEqI4xtIkOrAMrVVRUVKpGXVl8kcxam+iRg7g6nhnehmAfPVMW78PpUtcWqKio1C+qEFwEh0/l8/mvx7mth2cO4qoI8TPw/I1t2ZuSy8Jfj9eWeSoqKpcQi8VCjx496NixI+3atWPaNGVyJDk5mWuuuYb4+HhuvfVWbDYboEQr/f777y+JraoQ1BApJc8vPUCgSceTQzx3EFfFqI4x9GsZwZv/O0Jabv1FHVRRUakbjEYj69evZ+/evezZs4fVq1ezbds2nn76aR577DESExMJCQlh/vz5l9pUVQhqytI9aew4ns3TQ71zEFeFEIKXb0pASnheDT+honLFI4TA398fALvdjt1uRwjB+vXr+dvf/gZUHYp66tSpTJw4EZfLVS+2qmGoa0C+xc6MHw/TMS6YW7p57yCuirhQXx6/oSUv/3CIH/edYkSH6FprW0XlqmXVFDi1r3bbbNAehl14Uxmn00nXrl05duwYkydPpnnz5gQHB6PTKV1vbGwsaWlpFeo89dRT5OXl8emnn9bbvsvqiKAGzFqbyFmzlemj29XYQVwVE3s3oX3DIKYtP0Bekb1W21ZRUalftFptaUTRHTt2cOjQ+ZEEynf206dPJzc3l7lz59abCIA6IvCaEgfxhB6N6BAbXOvt67QaXh3bntHvb+G11Yd4dexfYuG1isqlw4Mn97omODiY/v37s23bNnJzc3E4HOh0OlJTU4mJiSkt1717d3bt2kV2dvZ5u5vVJeqIwAuUFcRuB7EXK4i9JaFhEPf0acrXO1LYnpRVZ/dRUVGpO86cOUNubi4AxcXF/PTTT7Rp04brr7++9O2gc0NRDx06lClTpjBixAgKCgrqzVZVCLxg2Z50diRn89TQ1nUeMfRfg+KJC/XhP2r4CRWVK5KMjAyuv/56OnToQPfu3Rk8eDAjR47k9ddf5+2336ZFixZkZWVxzz33VKh38803c9999zFq1Kh627e4TsNQ1xbdunWTO3fuvKQ2FFjsDHhrEzFBJpY8eG2t+wYq4+ejZ7hzwQ4eGRjPvwe3rPP7qaj8VVDDUCtc8jDUfzVm/aQ4iF+6iBXE3tK3ZQRjOjfkw43HOHq6/oaJKioqVxeqEHjAkVMFfLb1OOO7N6JjnHcOYiklZz/6iIKffqrRvZ8b0QZ/o47/LN6HSw0/oaKiUgeoQnABpJRMXbafAJOOp2qwgrjgf2s4M+tdUh/7N0W//eZ1/TB/I8+NaMuuEzn8d8dJr+urqKioXAhVCC7A8r1uB/EQ7x3EztxcTr38MsY2bTDExpL68CPYUlK8tmFsl4Zc2yKMN1Yd5lSexev6KioqKtWhCkE1FFjsvPzDITrGBnFrd+9XEJ9+402cOTnEDA0h7oH+SClJffBBnGazV+0IIXjlpvbYnC5eWH7AaztUVFRUqkMVgmp4t5yDWOulg7hw61byFi8m7I4JmDK+x7DnDWKnPYo1KZn0x59AOr17JbRJuB//GtSS1QdO8b8Dp7yqq6KiolIdqhBUwZFTBXy69Tjju8d57SB2FRWR8fw0DI0bE36D+7VPjRa/k+/T4JkpmDdtIvOtt7226d7rmtImOpDnl+2nwKKGn1BRudxp0qQJ7du3p1OnTnTrprzFmZ2dzeDBg4mPj2fw4MHk5OQA8MILLzBz5sxLYqcqBJVQsgdxgEnHk0Nae13/zOz3sKemEv3ydDTpW8EQAGPnwal9hDRMJeS228hesIDcxUu8alev1fDa2PZkFlh5Y/URr+1SUVGpfzZs2MCePXsoWQv12muvMXDgQBITExk4cCCvvXbpQ2CoQlAJy/emsz05myeHtCLUSwdx8b59ZC9cSPCtt+LbvTskbYLGvaHtaGh/C/z8JlH/GIFvr55kTJtG0a5dXrXfMS6Yib2b8OX2E+w6ke1VXRUVlUvPsmXLuOuuu4Cqw1B//PHHDBs2rN5WFqtB586hwGLnlR8O0SE2iPHdG3lVV9rtZDw3FV14OJFPPA55qZD9J3R3LyEf9jokb0KsfITYmUs5ftvtpD70ME2++w5DbEOP7/PEDa1Yc+A0/1m8j5UPX4dBp+q5ikpVvL7jdQ5nH67VNluHtubpHk9fsJwQghtuuAEhBPfffz+TJk3i9OnTREcrIeajo6PJzMysUGfOnDmsWbOGpUuXYjQaa9XuqlB7kHOYvS6RMzV0EGfNn4/1yBEavDANbUAAJP+sXGjaVzn6hsLIWXB6H9o/5hH74YdIp5PUBx7AaS70+D5+Rh3Tb2rH0dNm5m760ysbVVRU6o8tW7awe/duVq1axfvvv8/PP/9cbfkvvviCVatWsWjRonoTAVBHBBU4erqABVsUB3EnLx3E1qQkzr7/AQFDhxIwYICSmfwz+IZBZLuygq2HQ4db4Ze3MLYeQcN33iZl0v2kP/kksXPeQ2i1Ht1vQOsoRnaI5r31xxjeIZrmEf5e2auicrXgyZN7XVESYjoyMpIxY8awY8cOoqKiyMjIIDo6moyMDCIjI0vLJyQklO5f0LRp03qzUx0RuLkYB7F0uciY+jzC15cGzz1b0qDiH2jaFzTn/MxDXwPfcFj6IP7XdCfqP//BvGEDZ955x6v7Pn9jW0x6jRp+QkXlMqSwsLA0lHRhYSFr1qwhISGBUaNG8fnnnwPnh6Hu3Lkzc+fOZdSoUaSnp9ebraoQuFm+N51tSTVzEOd88w3Fu3YR9fTT6MLDlcysY1CQXjYtVB7fULjxXTi9H35+k5C/30bw+FvJ+mQ+uUvOdxxVRWSAiWeGt2FHcjbf7fJ+xbKKikrdcfr0afr06UPHjh3p0aMHI0aMKN1vYO3atcTHx7N27VqmTJlSoV6fPn2YOXMmI0aM4OzZs/ViqxqGGjBbHQx8ayORASaWTr7WK9+APSODpJE34tOxI3HzPynbXu63T+CHx+Hh3RDWvPLKS/4Jf3wL961DRiRw8r5JFO/aRaPPP8e3S2eP7i+lZPy8bRzKyOenx/sRGWDy2HYVlb8qahhqBTUMtRfMXpfI6XwrL41u55UISCk59eJLSJeLBi+9WHGP0aRNEBQHoc2qbmDoq+AfCUsfRAgXsbPeQRcTTepDD2E/Z0PrqhBCMGNseywOFy+tOOix7SoqKiolXPVCkHi6gAWbkxnfPY7OjUK8qpv/44+YN24k4tFHMMTGll1wueD4L8q0UHUbUPuEKFNEmQdh0xtog4OJ+/BDpN1OyoOTcRV69iZR8wh/Hr6+BSv/yGD94dNefQcVFRWVq1oISvYg9jPqeGqodw5iR04Op1+Zgal9e0LvuKPixdP7oDgHmva7cEMth0Cnv8PmdyBtN8ZmzWj4zjtYExNJe+pppMvlkT3392tOyyh/nluyn0Krw6vvoqKicnVzVQvBij8y+DUpq0YO4szXXsOZn0/0y9PPf+UzaZNyrMxRXBlDZoB/FCx9EBxW/PtcS9SUKZjXrePMO7M8asKg0/Dq2A5k5FuYuUYNP6GiouI5V60QmK0OXvnhIAkNA5nQw7sVxOZffiFv2XLC7rsXU6tKNqtJ3gThLSEw2rMGfYJh1Gw4cwg2KnFHQu64neBbbiHr44/JW7bMo2a6Ng7h9msa89nW4+xJyfX066ioqFzl1KkQCCEeE0IcEELsF0J8LYQwCSE+E0IkCyH2uFOnurShKkocxNO9XEHsKiwkY9o0DM2aEf7AA+cXcNjgxK+eTQuVJ34wdLodtsyCtF0IIWgw9Tl8e/Qg47mpFP3+u0fNPDW0FVEBJv6zeB92p2fTSioqKlc3dSYEQoiGwCNANyllAqAFxrsvPyml7OROe+rKhqoocRDf2s17B3Hmu+/iyDilRBY1VDKdlLYL7IXQrEwIzqYWYM6xXrjxIa9AQLQyRWS3IPR6Gr47C110NKkPPYzdgwUmASY9L45ux6GMfD75Jdmbr6aiolKLHDlyhE6dOpWmwMBAZs2adVWGodYBPkIIHeAL1N9SuSoocRD7GrQ8NdS7PYiL9+wh54svCZkwAd8uXSovlLwJEND4WgDMORa+f30X387YwZmTBdXfoHSK6DBsUqaIdCEhxH34AdJqJWXyQ7iKii5o55B2DRjSLopZPx3lRJbnMYxUVFRqj1atWrFnzx727NnDrl278PX1ZcyYMVdXGGopZRowEzgJZAB5Uso17suvCCH+EEK8I4SoNLKSEGKSEGKnEGLnmTNnas2ulSUO4qGtCfP3PKiTtNnImDoVXVQUEf/+d9UFk3+G6I7K6mFg+7IkpJRodRqWvL2btCM51d+oxSDofAdseRdSlUV0xubNafj2W1iPHCH9ac/eJHpxVAIGrYZnluzjSlg0qKLyV2bdunU0b96cxo0bX11hqIUQIcBooCmQC3wnhLgd+A9wCjAA84CngZfOrS+lnOe+Trdu3WqlJzNbHbzsdhDf5qWD+Oy8j7EmHiP2ow/R+vtVXshWCCk7oNeDSp3UAg5vP0XnQY3oMCCW5bP3suK9vdxwbzuadYqo+mZDXoE/18PSB+D+X0Bvwr9vXyKfepLM117nzOzZRP7rX9Xa2yDIxFPDWjN16X4W705jXNfYasurqPxVOTVjBtZDtRuG2timNQ2eecbj8t988w0TJkwAuOrCUA8CkqWUZ6SUdmAx0FtKmSEVrMCnQI86tKEC75WuIPbOQWxNTOTs3LkEjhhBQP/+VRc8uQ1cdmjaFyklW74/htFXR9dhjfEPMTH28S6Ex/mzeu4+Dm2tZpbMFKRMEZ09ChtnlGaH3nUXQX8bR9ZHc8lbsfKCdv+9RyO6Ng7h5R8OkmX2wEehoqJS69hsNpYvX87NN998wbKXbRhqIYRWSundTusKJ4GeQghfoBgYCOwUQkRLKTOEEo/hJmB/Ddr2mmOZBczfnMwt3WLp4oWDWDqdZDw3Fa2fH1HPXuAJIHkTaPTQqBcnD2aTejiHPjfHY/TVA2Dy1zPq0U6snref9QsPU2y20+WGxpW31WIQdLkLtr4HrW+EuO4IIYh+/nnsx0+Q8eyzGBrF4dOxY5XmaDSC18a2Z/jsX3j5h0O8c+sleUFLReWS4s2Te12watUqunTpQlRUFMAVG4b6mBDiTSFEW28allJuB74HdgP73PeaB/xXCLHPnRcOvOydyd5T3kH8tJcriHP++xXFe/cS9cx/0IWGVl84aRPEdsel82XromMERviQ0K/izmMGk44RD3agRbdIfl38J1sXH6t6Dv+GlyGwoTJFZFfmCoXBQMP3ZqOLjCTloYewZ2RUa1J8VAAP9G/Bkt/T2HS09nwtKioqnvH111+XTgsBV2wY6g7AUeATIcQ2txM30JPGpZTTpJStpZQJUso7pJRWKeUAKWV7d97tUkrzRX0DD/hhXwZb/1RWEHvjILanpZE5axZ+fa8j8MYbqy9cnAMZe6FZPw7/mkF2eiG9bmqOtpJtJLU6DYPvbkdC34b8vuYkG748jKuyd/5NgcoUUVYibHilNLv0TaKiYlImT77gm0QP9m9Oswg/nl2yjyKbGn5CRaW+KCoqYu3atYwdO7Y074oPQy2E6At8DQSjPO1Pl1IeqyPbSrmYMNSFVgcD39pEmL+B5Q/18dg3IKUkZdL9FO3aRfMVy9E3vMCewodWwP/djv3vq/jyExcBoSbGPdW1YkTSSu6xY2UyO384TrNOEQy+py06fSU7lK14FHZ9Dvesgbgyl0rBxo2kPvAgAYMH03DWO4hzN8Apx/akLG6dt42m4X48OaQVwxIaVGubisqVjBqGWqHWwlALIbRCiFFCiCXAu8BbQDNgBfBjzUyuP2avT+RUvsVrB3H+ihUU/vILkf/614VFAJRpIb0vew5HUpRn49pxLRBCIKXk9R2vM3XLVDac3IDFYSmtIoTgmhub0eeWeJL2nGHlnL3Yiit5Yr/hZSWkdbkpIoCA/v2JfPJJCtas4eycOdWad02zMD79R3f0WsGD/93NmA+2sj0py+PfQ0VF5a+LJ6+PJgIbgDellFvL5X/vHiFcthzLLGD+L8nc3DWWro09dxA7srM5PeNVfDp2JOTvt3lWKflnCqMHsfunNJp1jiC6hbLn8baMbXx56Ev0Gj1Ljy3FR+dDr+heDGg0gH6x/Qg2BdNxQBwmPz3rPz/E0nd+Z+RDHfENLLdq2RgAo9+DhaNh/cvK66VuQv8xEeuxY5z94EMMzZsTNGJElSZe3yqSvvERLNqVyttrj3LrvG0MbB3J08Na0zIqwOPfR0VF5a+FJ0LQoap5fCnlI7VsT63y4oqDioN4mHcO4tOvzMBZWFh5ZNHKyM+As0f4LeApXHYXvW5SdiSTUjJ792yi/aJZMnoJe8/sZf3J9WxI2cD6lPVohZYuUV0YEDeA69tdz7AH2vO/eftZ8tZubnykI4FhPmX3aNYfut0Nv74PbW6ERj0BZVTR4IVp2E6cIOOZZzHExeHToUOVpmo1glu6x3FjxxgWbEnmo41/MnTWz/ytayyPDW5JdJBPlXVVVFT+mnjiLH5fCBFc8kEIESKEWFCHNtUa/xrUktfGdSDcCwdxwcaN5P/wA+H3348xPt6zSsk/k+2I5eCxUNr1a0hwlC8A606uY3/Wfh7o+AB+ej96x/TmuZ7PsfZva/l6xNfcnXA3OZYcXv/tdYYuGsqTxx/AOfw45rxiFr+5m+z0c8JDDH7JPUX0INjKHMQag4HY92ajCw8ndfJD2E9feHMaH4OWyde3YNNT1zOxd1OW/J5G/zc38vrqw+Rb7B7/XioqKlc+F3QWCyF+l1J2vlBeXVLXexaX4DSbSRp5I9oAf5ouWoSoLKhcZSydzA+bmpMuu3L7y73w8TfgdDkZu3wsEsniUYvRaaoefJ3IP8GGk8ooYU/mHkIKoxl1eDJGYaLdHYFc16VbWf2kTbBwFPScDENnVGjHcvQoJ8ZPwNC0KY2//AKNj+dP9ynZRcxcc4Rle9IJ8dXz0IB4bu/ZCKPOgxGRisplhuosVqjNPYs17nARJQ2HUoehKS4lZ95+B8fp00RPn+65CEhJ2oE0jhd3ouuwJvj4K/VWJq0kKS+Jhzs/XK0IADQObMzEhIksHLaQ9bes55FBk0gZuJkCkcfeBTmM/+gent38LOtOrqO4UQ/ofi9s+0AJd10OU8uWxLw1E8vBg6T/5xmPdzcDiAv15d3xnVn5cB/axQQxfeVBBr61iWV70nC51FhFKip/ZTwRgreArUKI6UKI6cBW4I26Nav+Kdq9m5yvvybkjtvx6eT5ClyZlcSWjOH4+9npcL0Sz8fmtPHBng9oG9aWQY0GeWVHuE8441qOY9boN7h36iD8wg30338Hf+46w782/Iu+3/TlEaOFpZFx5Cx/oMIUEUDA9dcT+cTjFKxezdn3P/Dq3gAJDYP48t5rWHh3DwJNeh79Zg+j3t/M5sT6eZ9ZReWvwt13301kZCQJCQmleVWFoJZS8sgjj9CiRQs6dOjA7t27Adi4cSMjR46sc1svKARSyoXA34DTQCYwVkr5RV0bVp+4rFYynpuKPjqayEcf9aru0Z92ccbRgp5Do9AZlGmU745+R3phOo92fvSi3tWPCAvhzil9iWkWwnWHxzMj5APGxI/hYM4RpvpB/wAHE78fxsIDC0ktSC2tF3r33QTddBNn33+f/FWranTvvi0jWPlwH965tSM5hXZun7+dO+Zv50B6Xo2/j4rK1cTEiRNZvXp1hbyqQlCvWrWKxMREEhMTmTdvHg9UtulVHeJR0Dkp5QHgW2AZYBZCeBe68zLn7EcfYUtKosGLL6LxqyKyaCU47E62bTMRbjxJywHKmzpF9iLm/TGP7g260yum10XbZvTVc+MjnWiSEMbJH+3ckDWBNePW8M3Ib7jXtzl5had4c+ebDFs8jHHLx/H+nvc5nH2YqBdfwKdzZ9L/8wzF+w/U6N4ajWBM51jWPd6P50a04Y/UPEa+t5nH/m8PqTkX3hdBReVqpm/fvoSeE5amqhDUy5Yt484770QIQc+ePcnNzSXjnPAxv/32G507dyYpKanWbfUk6NwolOmhGJQRQWPgENCu1q25BFiOHCHr408IGj0K/+v6eFX3j/UpmC3+DOz6G0KraOp/D/2XbEs273Z+t9ZW7uoNWob+sz0bFh5m+/Jkis12+vytLe1G/5eHP+xNikWwvt9DrE/bzNy9c/lo70dE+0Uz9J5eDJl+kpQHH6Tpd9+hj4q88M0qwaTXcu91zbi5WxwfbvyTT7ck88MfGdzZqzGTr29BiJ+H/hQVlUvAL98e5WxK7UayCY/z57pbWnpdr6oQ1GlpacTFxZWWi42NJS0trfTz1q1befjhh1m2bBmNGtX+c7gnI4LpQE/gqJSyKUoU0S21bskloDSyaGAgkefE+7gQFrOdXT8m09i4k9juiqc+z5rHp/s/pX9sfzpF1m6kT61Ww8C72tBxQBx/rE/lp88P4tT5wk0fEJd1nLtSE/l82OdsuGUDL/V+iVYhrfhvxkqeGplLYU4m2+66iZ+O/ojNaauxDUE+eqYMa82GJ/ozulMM87ck0/fNDXy48U8s9poEqFVRUQEqDTxZ8iB56NAhJk2axIoVK+pEBMCzt3/sUsosIYRGCKGRUm4QQrxeJ9bUM9lffIFl3z5i3pqJLsS7vYt/+zEZu81Fr9CF0FSZB1ywfwFmu5mHuzxcF+YiNIJrb26BKUDP9mVJWIscDLmvF/oe98OOudB2FGFN+jAmfgxj4sdQZC9ia/pWfjV9Sb/3t/Pr00/w2oRo7m5/D+NajsOorVm885hgH968uSP3XteM11cf5vXVh1n463EeG9yScV1ivQrloaJS19Tkyb2uqCoEdWxsLCkpKaXlUlNTiYmJ4ciRI0RHR2OxWPj999+JiYmpE7s8GRHkCiH8gZ9RQki/C1zxISxtqamceXc2/v37Ezh8uFd1czOL2L8pjTZRhwmLMkBQLGeKzvDVoa8Y3mw4LUPq7h+eEIJuw5rQ77ZWnNjPjynyAAAgAElEQVSfxYp392Dt/SyENIVlk5Vd0tz46n0Z1HgQDz78GRGPPcq1hyQPLirm/Q0zGLZIcTIXO2q+FV6rBgEsmNidr+/rSWSAkae+/4Ph7/7C+sOn1e0xVVQqoaoQ1KNGjWLhwoVIKdm2bRtBQUGlU0jBwcH88MMPPPPMM2zcuLFO7PJECEYDRcBjwGrgT+ACMZkvb6SUnHr+eYRGQ4Npz3s9l79t6Z9otIIe2g+gWT8A5v4xF4fLweSOk+vC5PNI6NuQIfcmcPp4PkveO0LhwDmQcxx+eqHS8hGT7ifi3/+m7UEz8z7zYViiP2/+9gZDFw1l/r75FNprvsl9r+ZhLJ18LXNu64zF4eTuz3Yyft429qTk1rhNFZUrnQkTJtCrVy+OHDlCbGws8+fPrzIE9fDhw2nWrBktWrTgvvvu44MPKr76HRUVxYoVK5g8eTLbt2+vdVurXVkshNAC/5NSevcyfC1T2yuLcxcvIeOZZ4h6fiqht3kYVM7NqaQ8Fr2xi+7XGeiROAJu/oyURt0YtWQUY+PHMrXX1Fqz0xNSDmbz49x9+AboGdVpLUH734a7VkLT6yotbz12jPRnn8Wy9w+cvbuwYJietUW7CDIGcUebO7itzW0EGGoegM7mcPH1jpPMXpdIVqGNER2iefKGVjQJ9/xtLBWVi0VdWaxQKyuL3VtUFgkhgi7OxMsHx9mznH79dXy6diVk/Hiv6pbsQ+wbaKBTA7cqN+nLB3s+QKfRcX/H++vA4uqJaxvKTf/qjLXYwaLtAzjrey0sexCslb8lYWzRgiZffUXk00+j232Q+984yFeue+gU3pE5e+Yw5PshzPl9DnnWmq0XMOg03NW7CRuf7M8jA1qw/lAmg97exLRl+zmr7pusonJZ4snUkAXYJ4SYL4SYXZLq2rC64tTLryCLioie/lK1G7lURtLvZziVlMc1o5phSNkADdpz1JbFD0k/MKHNBCJ9a/Z65sUS1TSQsU90RavTsCTtCdIz/eCnaVWWF1otYf+YSLNlSzG1bo3u9bk89VUx/9f1PXpE92DuH3O54fsbmLVrFtmW7BrZFGDS8+8bWrHpyf7c0j2OL7efpN8bG5i9LlHdJU1F5TLDk57wB2AqirN4V7l0xVGwbh0Fq1cTPvlBjM2aeVXX6XDx65I/CY3xo3X3YEjZAU378d7v7+Gv9+eehHvqyGrPCI32Y+yTXfEN9mV57nSO//K7EqCuGgyNG9Po889o8MI0ivfsRXPn4zyf1p3vR35H39i+LNi/gKGLhjLzt5mcLa5ZiInIQBMzxrTnf//qS5/4cN5ee5R+b27ky20nsFe2PaeKikq949VWlZeK2vAROAsKSBoxEm1ICE2//w6h13tVf+/6FDZ/m8iIyR1o4rcfFo5m740zuX3/bB7u/DCTOky6KPtqi+ICGytm/87ZlHwGRn9FqymzlY1tLoA9PZ2M56dRuHkzPl27Ev3ydNJCJB/v+5gfk39Er9EzLn4c/0j4Bw38GtTYvl0nsnn1x8PsPJFD03A/Hr+hJcMTotGor5yq1CKqj0ChNreqTBZCJJ2bLsLeS0LmzLdwnD2rbDbjpQhYi+zs/OE4sa1DaJwQBkmbkBodszM3E2oK5fY2t9eR1d7jE2Dgpse7EtNYz08Zd7B37qce1dPHxBD38TyiZ8zAmphI8k1jCFq0gRm9prPiphUMbzqcb498y/DFw3np15dIM6dduNFK6No4lO/+2YtP7uyGXit46KvfGf3+FjWonYrKJcSTqaFuQHd3ug6YDXxZl0bVNoU7dpD7f/9H6J134tO+vdf1d//vBJYiO73HKvsQk/wzv8YmsCNzN5M6TMJX71sHVtccg0nHyCeuo1lMJpsPJrD98/UevdcvhCB47BiarVyBX58+ZL45k+MTbiPylIWXrn2JlWNXclOLm1hybAkjF4/k+S3PczL/pNf2CSEY1DaKVY/2ZebNHckutHH7/O3c/sl29qVeXUHtnC4nBbYCiuxFWBwW7E47TpdTXYehUq/UaGpICLFZSuldYJ6L4GKmhlwWC8mjb0I6nTRbvgyNr3eddn5WMV9N206LrpEM+kdbsOQhX2/ChPgO5BhMrBizAoO2+lg71qIipHRh8vOv0XeoKS5rEZumvcvB3Gto1zuCvrcneDwFI6WkYNUqTk1/GafZTPg/7yf8vvsQBgOnCk+xYP8CFh1dhEM6GNF0BPd2uJdmQd75XUqw2J18ue0E7284Rk6RnREdonnihlY0/Qu8cuqSLrKKs0gzp5FqTiXdnE6aOU1JBWmcMmfgkA6oZC2LRmjQCA1aoa30eN51TeXlypevroxeo8eoM2LUliWTzqQctSaMOuVo0BoqfDZqjRWv6UwYNIZai7VVEy6HqaG7776blStXEhkZyf79+wF44YUX+Pjjj4mIiABgxowZDHcvaH311VeZP38+Wq2W2bNnM2TIEI4fP87IkSNL61fHxUwNeRJ0rku5jxqUEcIVs9P52fc/wHbiBI0WzPdaBAC2L08CAdeMdndyx7ewzsfIAXs203tMr1YECnNz+G359+xdswqH3YZPYBChMQ0JiXanmIaERjckKCoanZfTVZ6gMfrS/4HrMc3+gt1bx2G1HqDjoDi0Og1arQatXijn5ZJGJxBCSYHDh+PbsyenX5nB2ffmULBmLdGvvEKDhHY8c80z3Nf+Pj478BnfHvmWlUkrGdJkCJM6TCI+xMMtPt2UBLW7pXscH/+cxCe/JPO//acY3yOORwbGExlgqvXfpraQUpJjzSGtII20QqVzL9/ZpxekYTLbiMiDiDxJRD7EmY10LdATnicJyHYhXILi2DAKG4VTGBdGQVwIBbEhFAeZcCFxSRdO6axwLEkV8l3nlzvv6HJil/ZKr9lddiwOC1anFavTisVhQVKzkYlAlApEiZBUJyDlhSfQEEioTyihprIUYgxBr639/yN1ycSJE3nooYe48847K+Q/9thjPPHEExXyDh48yDfffMOBAwdIT09n0KBBHD16tN5s9STW0Fvlzh1AMnBL3ZhTu1gOHiRrwQKCxo7Fr3dvr+ufOVnA0e2n6TKkMQGhSmfkTNrEe6EhNA1swshmlW8YUV4AnE4Hba+7nrDYRuRkpJGdnkbS7t8oyltbWl4IDYGRkYSWCkQsIdExhMbE4h8adlFPVqJxT3rdsALTT5+yddc/OLYr84J1NDqhCIVOg1Yn0PqMRdw4HFfmKcSM7Rgi/8TUqCFag44E3VBai4Ekm5P4MzmRtzZ8SWxQQzpFdyIqIKKcyAg07nOdQYNvoBG/IAN+wUa0OmWGMtCk5/EbWnFHr8a8t+4YX+84yaJdadzTpymT+jUj0HRpOoI8a17FJ3lzWWefXpCKIa+YiFyIyJdE5EFsgZ6OZgPheS4Cs21obRUD8mkCdOgbxqBvHYO+YUMQYDt2DMv+RJwby0KGa4OCMMbHY2wZj7FlS+U8Ph5tYGC9fG8pJQ6XA4vTUioMVqdV+exQjjanrfRzZWVKRaV8GaeFPGtexfpOa2mbVRFgCCDMFFYmDqaQCmJRPv9ymFrr27cvx48f96jssmXLGD9+PEajkaZNm9KiRQt27NhRGmYCICkpiXHjxjFv3jy6d+9eq7ZeUAiklNfX6h3rkVOvzEAbEkLU0095XVdKyZZFiZj89XQZ2rg0f0XqOpKMOt7u8sh5W1AqArCIvWtX4XTYaXvdAK4ZewshDc4PFGUpNJObkU52RlqpQORkpJFyaD8Oa9nCK53RWDqCKBlNhLpHE0ZfD6dOBjxH56PX0cg8g4IuU3C5NDhdQklOdyo9B5f7qKSScxPOqFiKktOwZaRjzs1FGx2DNJhwOiRBzkg6OMIpttlwZrpIPpLPCTwL/esToMc3yIh/cJk4jA8LYeSIYL49kM78dcf477YTTB7Qgtt7Nsakr919lIvsRZVO26SZ08jIT0OfU1D2RJ8H0QVa2psN7id6C1pHxddgtcE+6GNi0LdviL5hQ+U81n2Miam2I3dkZ2NNPIb16FGsiYlYExPJW74Cl7nst9Q1aFAmEPHxmFq2xNCsGRpT7Y6chBDotXr0Wj0B9TQJ4JIuCmwFZFuyybZkk2PJIduSTZYli+zibHKsyufkvGR2Z+4mx5JT6ahlVttZaLI16DQ6Dny7lLyUNGWkiyg9UnqujGAoed6SgJTuz+XyyxHZuBnXT6zZm4Jz5sxh4cKFdOvWjbfeeouQkBDS0tLo2bNnaZmSMNQlQnDkyBHGjx/Pp59+SicvdlD0FE+mhmYAb0gpc92fQ4DHpZTP1bo1tUzMjFewnzqNNsj7hdEn9meRdiSX625tidFH+Zlseal8oC2knbFBhS0ovRGAEkx+/jRo0ZIGLSoGqJMuF+ac7ArikJOeSmbSMRK3bUHKsk7HNyi4gkCUTDcFRzVAqyv39Kz3gZs+JOzToYT9Os7r36ICEWB2GMn4LQjHNi0hLQuJ7FCAxlT2nzFfI/gqMIAvA4IoFEZ6F9q5M7eYNlaJCz12TBQFdaUwuBuFppaYZQhFBU7MuVYyT+RTXGAvbasZ8BA+uAog86skXvv+OLEx/rRsEqwIR7AiIL5BBnQBYBFFFNgKyLflY7abKbAVlH4usBVgtil5+fZ8ciw5nM5LQ3M2t7STj8iDBvka2rqf6ANyrWicFTsabXgI+piG6FsoHXtJZ29wH/HxxW51Yit2YLM4sFmcmC0ObIVObPvM2C15pfk2iwOnXfk7Lel3EAEIukJ8V0S8kucqKsaVl4szLw9XXh7O3Fxcm/Ng025gNwjQBvijDQ5BFxKMNsR9DAxUFk6KEjeEKOvbRFmoYwRotQKtXotOr4zatHoNugqfy851eq37ulKutvwBGqEhyBhEkDGIpkFNL1je6XKSa80tFYySFGALINAQqDjeUQRGyoqSoZEgpERIEFK4P4OoUEogBcqPVy45LMVYCnLR6g1o9AY0Gq1Hv8EDDzzA1KlTEUIwdepUHn/8cRYsWFBtGOozZ84wevRoFi1aRLt2dbMNjCdTQ8OklM+UfJBS5gghhgOXvRAYGjfG0LjxhQueg8vpYuviPwmK9KFd37LO/Lud75Kh0/FC24kIIRQBWLGYvWt+xGm307bv9Vwz9tZqBeBCCI2GgLBwAsLCaZTQscI1h91O3ulTZGekklMiEhlpHNu5neL8vAptBEVGnSMSsYTeuRE/TZHyD126qkjVXVOu+0sXzYqKOfPVT+Ss2Yk5N5boe4fi1zYOpItAKfmndHGH08I3Ofv4PGcv9wZKepii+WdIB7oJP0TKdkhdAi4HaPTIuO4UtelNQcPO5AU1JTuvmNxsM/k5RRTm2bDmO9Fk2XHmSE6l5WI+kY3edb5/xqototCQT5Ehj0JDLoWGfOWoz8duKKBhoZVWp4rpmWKjYZoF/1wbGncH4dLocep8kJExiKhYZMto7KFRuALDcAWE4PINxKn3xe4QSkde7FA6/BMObEec2CynsVnScVg925tBZ9CgN+nQuafGJJKSPki6n0qlcsF91APhSGM4RIKMkOByIZ1OpNOFdLmQDheclpBpR3IGOAsajbsDU47KLUSF9i+WElHQ6TVoDdrSc537XFvFNW1V5UqOOm2ZL0tfToDc/iytRkuYTxhhPmEV7Dl06BDRvg2Qdjsx4ychbTakzYbLZkNarUi7vUJ5qRG4dBocOg12Hdg1ym+rcUq0TonOCVoX6JzlBg4nUkvDMDs14NQInFqBSyuQOg1Sp+HMmQwcTjun89MRWh3aQC159jy0QsuEuyZwy5hbsDgsRMdEc+LkCVzShUZoSsNQAwQFBREXF8eWLVsuqRBohRBGKaUVQAjhA9QskP0VwqGtGeRkFDLs/vZo3TuPFdmLmJe2ju5WBx1iR7Dxi/kVBWDMLYREN6xTu3R6PWGxcYTFxp13zWI2lwpD+ZFEyoF9OGxlU00BYRHEtU0gtl174tp2ICgyqkZPc1qgQc+7CPztN9Kfe46TM74h+JZbiHzyCbQByjSCH3APMMFexHdHv+OzA59xd8b/aBvWFv+GDSkI9yO/6CxmWwEFrlRcKd9CyreV39AEpkYm/Fv4I1w+5Ji1OIv8iNE0oGNwLDHacIxWP4IsvoQURkBRNI58sJ91IeU530+6yA4uxBwJTq0RBzrsDg3nPZRZgHR3wgnkIDS5GExaDCYdBh8teqMOk7+ewHAfDCYteh8dBqMWg48Og0mH3lR2bjBplc/uc43WuxAnnuIqLsb6Z5IytVRuislx+nRpGU1AwHnTS5rIaKTRB5fehFNqcDpcOGwuHHYXDrsTZ7lzh82Fs+Tc7qrimgun3Ym12IEjz92GvaRNpd7FipBWr0GrkWiERONyoHHZ0NgstL6jMdmGLEC6H3wkQmgQGh+Ejx/CX4vQupNeOWoF6ITA5B4xCVE2LSRxlf1xOpBOO9LuQDrs4HAgHA6E04Xe4URjdaEpUh4GArKL0TpcBJ7MRgJp2WcIj4rAroVFCxcS36wxpzOO0bV3Ao899DQj7hxOZuYZDh45SEh8CCfST6A36Fm6dClDhgzB39+f27wMlOkJngjBl8A6IcSnKH9tdwOf17ollwk2i4PtK5KJbh5E007hpflfHvqSIouTYcdbMP/R+3Ha7bS5rj89x95a5wLgCSZ/f6LjWxEd36pCvnS5KMg+S056OllpJ0k7fJDkvbs5+MsG4OKFwbd7d5otXcqZ9+aQ/dlnmH/+megXX8C/X7+yMnpf7mp3F7e2upXFiYtZmbQSu8tOhF8DmoXEE6APIMAQQIDQEZB/ioDsZAJOHyYw+zj+LhcBen8C4nphaNYfmvaFyDbYXZJvd6bw7k+J/F+qlUFtInmiezgxKUcp2v0bxb/vwnL4MNIlsRkDoVUnXC074GrYAltQNBaHDmuRA71RW9aBl+uk9SaduwPXVujQdbU4DVJXaHx88Eloh09CxadHZ24u1mPHsJQTh/wfV+HK/7/z2hBGIxp/fzT+fmj9/NH4+aH190dfkuev5Gn8/JVygeXy/P3R+Pmj9fdD+PpW+XtJKXE5pSIgtjKRcDqUpAiJC3uxFdvps1hPn8V6JhtbVg72nALseQXYCy24NFpcGj0ujR5p8AG/QIRGIAx6EBqkULwALuWmyqDXCTgk4HIne6U2Vo3enXzcP5hA6EDo3ecC7ps8ka2/biYrO4vmg4cw5d9Ps/nXzew/uB8BNGoYy9svvU6QJYResd342w2jGdP7JnRaHW9PfZnIfD+K83UgJX5+fqxcuZLBgwfj5+dXuo9BbeHROgIhxFBgEIo+rpFS/s+jxoV4DLgXRUD2Af+QUlrc195zf77gy/W1HYa6OnasSOK3H44z7qmuNGim+BZOnTnJtHcm0iLZhFZqaHPdgMtGAGqClJLstBRSDuwj5aCSSqaWAsIjiGvbXknt2hMY4bkwFP/xBxnPPos18RhBo0cR9Z//oA0OrrmhBafh+C+Q/LOScpKVfL8IZOM+2EwJ5GWa2L/5EGL/HzQwu1cnG034duqIb9cu+HTpik+nTmj9L4M1CU4H5KdB7gll74gc99FmBlMQmILBJ1g5moLKzn2Cy64b/Cpdc1BTpJQ4MjOxHj2K48xZXGYzrqJCXGYzTrMZl1k5dxUW4iws99lsRto82PZUiDJxKBGV8oJRKiolwuKL42wWtpMnsJ84ge3ESewZGeAq841pAgOVad9GjdzTv8pR37gx2uBghBAerSOQUpZOu5Weu4WC0iNlvgVZvlzZ9pKVlndXOD+vkjbKXauMkAgjet8LT8JczDqCCwqBEKIpkFGuA/cBoqSUxy9QryGwGWgrpSwWQnwL/Cil/EwI0Q14FBhzOQlBYa6VL5//lSbtwxlyXwJFebn8tmIxO1cvxWV30ijkLDc8NJOQhMpj/V+plAjDyQN/kOoWh+KCfOB8YQiKrD7OkMtmI+ujjzg772O0wcE0mDqVwCE3XLyNdjuW7Rso2rCcot/3UJx0FqdF6RC1JjA2DeNEdHM+pi07g9vx997NmHx9C0L8ql/sV6tICcU5imCVdPLlO/28FMUnUoLQQlAsmALBkg+WXOVY3XyJRleFUFQjHiV5xkDFX1BbX9dmw1lYiKuwTBycbtEoE5BK8sxmRVQKi0o/n9sTlnb2JR1+E+VYvrOvjsthQZm3VCYsUoLWvbbnQtS1EOwEekspbe7PBmCLlLLaF1ndQrAN6AjkA0tRwlOsA34CbgMSLych2PDFIQ5vO8WYx9twZOuP7FnzA06bjaSGRQTFu3jFnAaPH6nVJ7LLESklWaknSTm47zxhCIyIJK5te2Ld4hAUGVVpG5bDh0l/5hmsBw8RMGQIDaY+hy48vNKyleE0mynes5fi3bso2rWb4r17kRblHXN9o0b4du2Cb6tYfEILMRTvQxzfDMVKyOxMQxxri1uyW9OBNr2Gc9uALvgaPJkF9QC7BXJPntPJHy/3dF9QsbxvOIQ0hpAmSgouOW8MgbGgPcculxOs+WDJg+JcRRxKjufl5VW8XpzrnvOoCqGITlXiYQxURhylyd999C137s6vxcVdUkpkURFOsyIq2pBgr/cQP5crUQgulroWgj1Syk7n5O2VUnasqk65co8CrwDFKFNKf3fnaaSU7wghzFUJgRBiEjAJoFGjRl1PnDhxodtdFFlpZr55aSMhkUc4c2IrTpudNn36sbt5Nt+f+ZHlZwuJa9wXxn1Sp3ZcjkiXi6y0FFIO/KGIw8H9HgmDtNvJWvApZ+fMQePrS9RzzxI4cmSlTzf2zEyKd++maNduinbtxHr4iDIdoNFgatMGn65d8O3SFZ8undFHVrLvg8sFmQdKp5GcyZvR2pV374/SGGfj64jvORxd0z5gCsLpcGC3WtAbjWi0ujKbXC4oyDh/+qak4y/IqHhfnU9ZR1++ky/5bKzHsCJSKntWeyMe5a87ql7MdR5agyIKej/PxcPgB/pz80uu+SptXeyIxf0YfejwYXenWK5/k+edUMlF5VSgjNiuoIe+uhaCtcB7Usrl7s+jgUeklAMvUC8EWATcCuQC3wGLUTr3/lJKR3VCUJ66HhEU5eXyzYtzyUnfhsBJ6z796Dn2Vgr9JaOWjmJc3ACe2/QJjJoDXe6oMzuuFKTLVTpiUNJ+LKXCEFU6jRTXtj2BEZFY//yTjGeepXjvXvz796fBC9NwFRVRtGsXxTt3UbR7N/aUFACEjw8+HTvi26ULPl274NOx+vl96XJhLSqi2JyPxVyApaCAYnMBlrxcik8lknXiCPmZqRhshdhcWoqdeiwuIzZn2X9wIUCvBb3GiR47euFQzjUudMKF3mhE7+OH3i8QvX8w+oBw9IER6IKj0QdFoDf5KGWMJvQmE3qDUTkajeiMRjSa2l0AV2c47YqQlCZzxXN7USX556ZzrtkLldeOPUXnAxot5SbXKzlS+bVyHBryLW0a18JGURrd+Ulbcq4/59qlFY46jTUE/BP4rxBiDopOpgB3Vl8FUJzLyVLKM26DFgMvorjZj7mfwHyFEMeklC08aK/WKcrPY+eKxexetQKn3U5U8+4Mf+huQmNiAXjjlynKFpQGt1O4ad9LYeZlh9BoCG/UhPBGTeg89MbzhOHP3Ts4sOknoEwYYv95D4FHEin86GOO9S9brK4NDcW3axdCbrsNfcf2EBODxVJMUUE+2WYzll83UVxQgMWcj8VsprggX+noCwqUjt9srrDIrqKhApOvH6aARpj9jGQXFGKS+TQy5BKjOYNBOHBIDQ6NL3Z9MHZdAHatL3Zhwi512FxaipwSu9WG3WzBnmXBbklDyhSvfi+d3oCuRCCMJSJhKhUPg68f/iEh+AaH4B8cil9ICH7BIfgFh6Iz1KOPQ6tXpol8LsLBfy5SKiONagXkHMGRsqxDdb/xU+WxqjKmIAiIrsyicvUqySuPywkuu+LXcTnAXqwcq5x+E1UIRjnhKJ/nXtdxOeBJiIk/gZ5CCH+UEUSBEKLyieGKnHTX80WZGhoIvC2lfK+kgHtEUO8iUCIAv/9vJU6bHZ+gdhgDezH+xRHo3OELjuYc5cekH5mYMJGIozvLhvwq51GZMJxNPUnKgX2knisMvToQihZ8fbAbDFgddiwFeVjWLcex+vsq76E3mjAFBGDyD8DHP4CAJhH4+Ls/l+QHBGLy98fkH4hPQABGP78KT+NOl2Tp72m8vfYoObk5jGniYMr4gQQEe+67kFLitNux26zYLRbsVgsOa9m5kqzuZCmXb1XKWcvKFeXnY7dmYi00U5SXV6mgGf388AsKwS8k1C0OFc/9Q0LxDQ7B5Od/eb7WKoSysl3vA36e/84XzaFDEFDzDZSqRboUQXA6ykSifHK6xcNmdQtHVSMiUU4w9FULiM6nVp38leGNB00LjBNC3Aa0Aap9d1JKuV0I8T2wGyVY3e/AvJoaWhsU5eexc+US9qxeicNmo/W1fYlsPoDty3Ppc0vbUhEAyragbHMXrJkF7W66hJZfWQiNhohGTYho1IQuw84XhtPJx9AjMRkMBIWG0aB5fIWO3Mc/EFNAQGlHbwoIrJXorFqNYFzXWEZ2jOaLX0/w2qrD/PltIp/9I8Tj+EVCCHQGAzqDAR//2ou/43I5Kc7PpzA3h8KcbMy52RTl5mLOyaYwN5vC3Fwyjh2hMCenwgLB0u+m1yviEBTiHlG4xeLc86AQNNr6napS1gs4cTrsOO3u5LDjcJ+7HA4cjrL82ljp7NDqsRYW1rwBofw71mh1aLRaNOU7YqFRfCQXCD+fkpLCnXfeyalTp9BoBJPunsijk+/nhemv8PGnXxARFgpIZjz7OMMHXgsOC6/O+oj53yxFq9Eye/qTDOnfm+OFvoz8//buOzyu8k70+Pc3XaNRtSQ3WcW2bOMuY4xtHAIYWByMCZgakgAhgSQEZ5Ns7pLc+yzJZu8uyd3kWSCksAktEAimhlBC2dgQV4wlF9yR5M8Ls2IAACAASURBVCZZxSrWaDSa9t4/zmgk2SqjMpqR9H6e53jOnDln5tVIPr9z3vJ7r1sbVSrqgeo1EIS7iq7B6OGzCCP99Ocx5i/uk1LqAeCBXl4flpa0zgHA72vjvIs+y4XX30xa9iSefWArOfkpFJ3fcZNTWlPKhuMbuK/4PtIaKqCtSVcLDcLZgSHe7BYj7XWWy84//qmUdc+V8KvbFmGJ0UjfaJhM5shVPgU9z+uglMLX2moEh4Z6I3A0NuBuqMfT2IC7sYGGqkpOHNgXabfpQoSklFRc6eGqqE53F2arjWAg0HHC7uHEHQz4jf267NNxXKDTce0n+V47ysfAinu/T8OpyiF7PyMomCOLuT1ARJaO52IyBh1aLBZ+/vOfs2jRIpqbmzn//PO5YtVqsDr5zne/130q6jc/4JN9B6k8cYzL/+FzHNp9K3gbhuzn6EmPgUBEngUuBt4Bfgn8D3BEKbUh5qUaIp4zTXz8l1coCQeAWcsvZunaWxg32UjR8PHbFbgb2rj8ztlIeMIWpRQPlzzcMQXl1t8Yb1b42Z4+RhuhPl88mQaPjx+/vo///cpeHlw7LzGrVzoREexOJ3anM9KW1ZOA34+nqSF8l9EQvrMw1t2N9bQ0NHD65HE8jQ2Egt3Xe5utVswWq/FotWJpX7dYMVssmK1WbElOzCmWLvtawusmiyWy3v4eZ7+fKfw+7e89FL+DOo838v98IBQKFQoRCgQJBgOoYJBgMGjc2fgD+L3eHr8zEcFkNmMzmymcOJ6mmmpMZjMzpk+n7PAhI6AGAgQDASNwhH/eSCrqJCeFRbOYXlTE9j0HhyUVdW93BHOBBmA/cEApFRSR+Cf57ofX/t+/UXn4gBEArr+lS46e1mYfH799lIL5WUye0dFneUvVFj469RH3L7nfmIKyfCPkzAFXdjx+BC3G7ryokAaPn4ffP0xGso37V82Kd5GGjMVqJTUrh9Ss3nvPqFCIVnczoUAgfKI2TsxdutWOMA3792MNp+VufP1TfJWDqCbqhm1SMmmrpxMKB4eOJXDO84DPR8XRo5SUljIzP58N77/PU089xVNPPsmCeXP58f/532RmZvLpoYNcsHgxZ+pqMZnNTMjJoeLTTxk/3vj9xTIVdY+BQCm1QERmYVQLvSciNUCKiExQSp0a0lLEyCVf/iq2JGe3Sdo+eqOCgC/E8uunRbYppXh458NMSp7EjTNuhEAbHNsK5985nMXWhtl3Li+iocXHbzZ+SobTyj2fndb3QaOImEw4U/ufqn2sExEjaFp6b2p1u918/fobeOjhhymcPYdvf/d7/PgnP0GFQvz4J//GT376Mx75xc8jHRFam8+gQiH8Xi8tTY2EAsGYp6Lu9SdQSh0A/gX4l3BaiFuB7SJyQinV/ym/htnZCdjaNVZ7+OSDk8xZMYmMCR191N879h6fnP6En1wUnoKy/EOj69tUXS00mokIP1ozhwaPj/946wAZyTZuWjzwagUtsaRfE7/A7vf7Wbt2Lbfddhs33HADAFMKCiKv37tuHatXryZ9wiSmz5xFU6uX8YXTCIVC1DU1MWPOXMwWS8xTUUfdOqaU2qGU+h6QD/xgyEsyjLa88ilmq4kLVndMfBEIBXik5BGmpk3lmqnhBs3yjcbowvyL4lRSbbiYTcIvblrIZ4qyuP+l3fz1kxFx06slMKUUd911F+eddx7f/e53I9urqjpGp7/yyivMnTsXgDVr1vD888/T1tbG0aNHOXLkCBd95jOIyYTNZuPVV1/l6aef5o9//OOQl7XfCViUMRR545CXZJhUHmmkrLSWC9cU4kzt6P71l7K/UN5Uzi8u+QXm9r7n5R/ApGIjP4s26tksJn7zxfO57XfbuO+5Ep66cwnLpo3r+0BN68amTZv4wx/+wLx58yJ1+v/+7//Oc889R2lpKSJCQUEBv/3tbwGYM2cON910E7Nnz8ZisfDoo49i7tTVN5apqKNKQx1vQ5ViQinFSz/7GHe9l9t+sgyrzfiSfUEfq19ZTaYjk+eufs5oIGtrhp8WwEXfhpX/MujP1kaOhhYfN/12C1VNXp6/eylzJ+v685FGJ50zRJtiIn4dp+PgyMc1VJef4cJrp0aCAMD6Q+upaqli3aJ1Hb0kjm4xRgXq8QNjTkayjafvWkJakpXbH99OWa2774M0bQTrMxCIiF1EviAiPxSRf2lfhqNwQynoD7H11U8ZNzmZmUs7+uV6/B4e2/0YSyYsYdnEZR0HlG8Esx2mXBiH0mrxNjEtiT/ctQSAL/1+O6ea+pGZU9NGmGjuCF4DrsVIE9HSaRlR9mw8wZk6L8vXTsdk6ugb/cz+Z6j31ne9GwAo2wh5Fxo5UrQxaWq2iyfvXEJTq58vP76NRk8UM3Jp2ggUTSDIVUrdrJT6mVLq5+1LzEs2hLwtfna8WcGU2Znkze5o/Gtqa+LJvU9yyZRLWJDdaXqFltNQvWdIqoWUUtQ9sZf6Px0k5A30fYCWUOblpvHYl8+nos7DnU9+hMenf4fa6BNNINgsIvNiXpIY+vjto7S1Blh+fddEp7/f+3vcfjfritd1PaAinEqp8JJBf3bbkUa8BxvwlNRQ88tS/KdG3M3UmLd8WhYP31rMruONfOOZnfgC/civr2kjQDSBYAXwsYgcFJHdIrJHRHbHumBD5UxdK7v/dpxZyyaSlduR467GU8Mf9/+Rq6deTVFGUdeDyjaCLcXoOjpI7k2VmFxWsu6aS6gtSM2jpbR8XD3o99WG11VzJ/Af189j46Favrd+F6FQ4ve207RoRRMIVgFFwJXANcDq8OOIsPW1MkwiXHhN14yOv931W4KhIN9c+M1zDyr/AAouOnc+2X7y17XiPVBP8oUTcRRlMH5dMba8FBrWH6L+xUMof2/zy2qJ5uYL8rh/1Sxe31XJj17/hJHQ9VqLr4KCgsg4gsWLjV6c9fX1XHHFFRQVFXHFFVfQ0GBkF1VKsW7dOqZPn878+fPZuXMnABs2bGD16tUxLWefgUApdRRIxzj5XwOkh7clvOqKMxz+qJqFV+ThyrBHth8/c5yXD7/M2hlrmZJyViqBphNQ/+mQZBtt2VwJZsEV7qVkTrGRddc8Ui6bgmdHNTW/2oW/rnXQn6MNn69/dhp3XzyVp7cc5aH3D8e7ONoI8Le//Y3S0lLax0I9+OCDrFy5ksOHD7Ny5UoefPBBAN566y0OHz7M4cOHeeyxx/jGN74xbGWMpvvot4FngZzw8oyI3Bfrgg2FLa8cISnFSvGVeV22P7rrUWMKyvn3nHtQWXjQ9CDzC4W8AVp2VOOcn405pWMEs5iEtCsLGHfnHIJNbdQ8UoJnT92gPksbXj9YNYsbz8/lv947zFObK+JdHG2Eee2117j99tsBuP3223n11Vcj27/85S8jIixdupTGxsYu6SgAPvroI4qLiykrKxvSMkVT93EXcKFSqgVARH4KbAEe6fWoBLDixiLcDW3YHB0/ZvsUlHfOvZNsZzeppcs/AGcWZA9uVGLLR9UoXxDXRZO6fT1pZibWdcXUP3uA+mf347toEmmrChHLmBrjNyKJCP9x/TwaW/088OdPSHdauXZhrxP2aXH01ltvcerU0OaOmjBhAqtWrepzPxHhyiuvRES45557uPvuu6muro7MMTBx4kRqamoAOHnyJFOmdNRQ5ObmcvLkycjzzZs3c9999/Haa6+Rl9f14nawogkEAnSuzA7S7UzPiScrN4Ws3K7TCT6y05iC8itzv3LuAUoZA8kKLx7UHKEqpHBvqcRWkIott+fpDC3pDrLvmU/TW+W4N1XiO95M5hfOw5Ju7/EYLTFYzCYeubWY2x/fzvde2EVakpVLZvae918bezZt2sSkSZOoqanhiiuuYNasnue76K7NqX1s0/79+7n77rt55513mDSp+4vLwYgmEDwBbBORV8LPPw/8fshLMgxKa0rZcCI8BaW9m/wxdYehuWrQ1ULe/acJ1ntJW1XQ575iMZF+zTRs+ak0vHSYmod3knnzTBwzMwdVBi32HFYz/337Ym59bCtff+Zjnv3qUs7Pz+j7QG1YRXPlHivtJ+2cnByuu+46tm/fzvjx46mqqmLixIlUVVWRk2NcQOTm5nL8+PHIsSdOnGDSpEkcPGjMUub1eikpKYlJIIimsfgXwJ1APcaMZXcqpf5ryEsSY+dMQdmd8nD7wCAHkrk3VWJOt5M0OyvqY5zzs8n51kLMqXbqnvyEpncqULqLYsJLdVh58s4lTEh18JUnP+LgqeZ4F0lLEC0tLTQ3N0fW33nnHebOncuaNWt46qmnAHjqqaciWUTXrFnD008/jVKKrVu3kpaWFqlCSk9P54033uCHP/whGzZsGPKy9hgIRCQ1/JgJVADPAH8Ajoa3jShbKo0pKO+ef7cxBWV3yjdCWh5kFHb/ehR8lW7ayppwLZuEmPtXg2bNdpL9zQU4F42n+X+OU/f4XoJundYg0WWn2PnDXRfisJr40u+3cbzeE+8iaQmgurqaFStWsGDBApYsWcLVV1/NVVddxf3338+7775LUVER7777Lvfffz8An/vc55g6dSrTp0/na1/7Gr/61a+6vN/48eN5/fXXuffee9m2bduQlrXHNNQi8hel1GoRKQc67yQY0xJM7fbAGBhsGmqlFLe8cQuN3kZev+51Y/axs4VC8LNCOG81XPvogD+rfv0hWnfXMvEHSzA5rQN+n5Ydp2h49VNMSRbGfWEW9kKdCjnRHTzVzE2/3UKG08r6ry8nO0W39cSLTkNtGHQaaqXU6vBjoVJqaqelcDiDwFB479h77Du9j28s/Eb3QQDg1G7wNg5q/EDQ7cOzqwbnopxBBQGA5MUTyPnmAkx2M7X/vZvmjcf1AKYEN3NCCo/fcQHVZ9q444ntnPH6410kTYtKNOMI3o9mW6LqdgrK7gxB+0DLtlMQULguGpquhLZJLnK+tZCkOVk0vVXB6af3EfLok0siOz8/g19/cREHTzXztad24NWjx7URoLc2Ake4LSBLRDJEJDO8FABD32wdI69/+jrlTeXcV3xfxxSU3Sn/ALJmQsqEAX2OCoRwb63CPiMDa04PbRADYHJYyPzCLNKumYr3YAPVvyzFd0I3SCayS2bm8PObFrC9op77nishENRJ6uJhLN1BD/Zn7e2O4B7gY2BW+LF9eQ0YeCX6MPIFffx616+ZM24OK/NW9rxjwAdHNw+q22jrnjpCzb4eB5ANhoiQctFksr8+H4KKml/vwr21akz9oY801y6czI+umcO7+6q5/+U9+nc1zBwOB6dPnx4T37tSitOnT+NwOAb8Hj2OI1BKPQQ8JCL3KaUSfhRxd9qnoPzR8h91nXTmbCd3gN8z4PYBpRTNm05iyUrCURS7fuT2vFRy1hVT/6eDNL56hLaKJjKuK8Jk7+VOR4ub25cXUN/i46H3D5OZbOOHn0vMxktfIMTBU82ElKJgXDJpg2zfSgS5ubmcOHGC2traeBdlWDgcDnJzcwd8fJ8DypRSj4jIXGA24Oi0/ekBf+ow2Va17dwpKLtT/gGIycg4OgC+Y834T7hJv3YaYortoGtzspWsO+bQvOE4Z949ir/SzbjbzsM6Pjmmn6sNzD9eXkSDx8djH5SRmWzj65+dFtfyKKU40dBK6fHGyLLnZFOXORbSnVYKxiVTmJVM/jhn+DGZwhEUJKxWK4WFA+8GPtb0GQhE5AHgEoxA8CZGWuq/AwkfCB669CGa/c293w2AkWhu4gJIGtjVvHvTScRhxrlo/ICO7y8xCamX5WHLS6X++QPU/LKUjOuLcBbrFAeJRkT40TVzaPT4efCtA2Q4rdx8wdDmielNs9fP7hNNlB5vpOSYceKvc7cBYLeYmDs5jS8tzWfhlHTsFhNHT3soP93C0dMtbC+v59XSk3SuXWkPEgXjnBRkJRvrWcbzdGcPPfK0hBdNiokbgAVAiVLqThEZD/wutsUaGiJCqi219518LXDiI1h274A+I9DYRuveOlwXTR72KhrH9HTGryvm9B8PUP+ng7RVNJG+ehpi1YnrEonJJPznjQtobPXzg5f3kJZk46q5A+uU0JtgSHGoujl8wm+g9Hgjh2vckRP51KxkLi7KYmFeOsVTMpg1MQWrufe/Fa8/yPF6DxWnPVTUtVBx2lg+qmjgtV2VXYJEWpKVgqxkCsc5jTuITncUOkgktmgCQatSKiQigfBo4xogqnEEIvId4KsYA9L2YKSqeBRYjDEw7RBwh1LKPZDCD4ljWyDkH3C30ZatlaDAtSw+HanMqXayvzafM+9U0LzxBL4TbsZ9YRaWcUlxKY/WPZvFxG++uIjbfreNdc+V8ORXLmD5tOhTkHSn+oyXkmONlBxvoPSYUcXj8RndVdOdVhZOSedz8yaycEo6C6ekD+hk7LCaKRqfQtH4c5Mnev1BTjR4KK/zcPR0C+V1LRw97ek1SBSMc4bvIsKP45LJSNZBIt6iCQQ7RCQd+G+MXkNuYHtfB4nIZGAdMFsp1SoiLwC3AN9RSp0J7/ML4FvAgwMs/+CVbQSzDfL6aEfoRsgXpGX7KZJmj8OSOfAW+8ESs5C2qhBbfir1Lxyi+pESMm+cSdKccXErk3Yup83CE3dcwE2/3cLXntrB83cvY15udCPGW31B9pxsovR4Q6SKp6rJC4DVLMyemMqN5+eyMC+dhVMyKBjn7LtKdJAcVjPTc1KYnnNukGgLhO8k6jyRu4iKOg8fH23gz90FiXBVU/64ZCamOch22clOsZOVYifLZcNu0R0iYimaxuL2uRx/IyJvA6lKqWjnLLYASSLiB5xAZacgIEASXdNXDL/yjZC7BGz97/vvKakh5AkM2QCywUqaPc6oKnp2P6f/sA/XxZNJ+4cCpI/bf234pDttPP2VC1n7683c/sR21n99GdOyXV32CYUUZXXu8NV+I6XHGjlY3UwwnIRwSmYSiwsyI1f6cyal4rAm1onSbukrSLR2qWo6etoIEq/vqqS7XItpSVayXDayU+xkpzg61l1GsMh22clJsZOZbMOi/977rbdcQ4t6O1AptbPPNzdmN/u/QCvwjlLqtvD2J4DPAfuAq5VS52TpEpG7gbsB8vLyzj96NAazY3rq4WdT4ZIfwCX/3K9DlVJU/9dOxCTkrCuO+dVXfyh/iMY3ymjZWoWtIJVxt87CnKbz3iSS8roWbvj1ZhxWM7+7fTGVjV178jR7AwCk2C0sCJ/wF05JZ2FeOlmu0fu79AVC1LnbqG1uizxG1iPbfdQ2t+FuC5xzvAhkOo0gkdV+VxEJIB3bsl12Mpw2TDHu5Rdv0eYa6i0Q/C286sCo09+FUa8/H9imlFrRRwEygJeAm4FGYD3wolLqmfDrZoxZzj5SSj3R23sNNulcj/b9GV74Enzlr5C3tF+Heg83UPf7vWTcMIPkxcPTW6i/PKU1NLx8GLGaybxlZkzHOGj9t/dkE7c8tjVyQjMJzJyQSnGecdIvnpLOtGzXqD9ZDVSrL0idu42a5rMCh7uNuuaOwFHb3EZb4NzR3WaTMC65a9BoX89JsTM1O5lp2a6Eu9vqj2gDQW8Dyi4Nv9HzwN1KqT3h53OBf4qiDJcD5Uqp2vBxLwPLMdJZo5QKisifgO9jTH4z/Mo/AGsyTD6/34e6N1VicllxLuhmussE4VyYg3WSi9PP7Kfu8b2krswj5bK8mI910KIzd3Ia67++jM2fnmbupFTm5abhtEXTbKcBJNnMTMl0MiWz92pdpRTutkCXu4naZm/HutsIIoeqm6lzt+EPdlwcmwTyxyUzY7yLGeNTIkthVjK2UTStbDR/dbPagwCAUmqviCyM4rhjwFIRcWJUDa3EaHierpQ6Em4juAY4MJCCD4nyjZC/HMz9GyTjr2vFe6CelJV5Cd9V05rjJOdbC2l8+TBn3juGe1sVzgU5OItzsE5KTqgqrbHovImpnDexjy7O2qCICCkOKykOK1P7uG5TStHU6ufUGS9HatwcqnZzuLqZg9XNvLuvOtJ+YTEJhVnJnYKDi6LxKRSMc47INopoAsF+EfkdxpW8Ar4I7O/rIKXUNhF5EdgJBIAS4DHgf8LdUAWjuukbAyz74JyphLpDsOjL/T60ZXMlmAXX0okxKNjQM9nMZNw8k6T52bTsqMa9pRL3309iyUnCuTAH58KcuPZ60rREISKkO22kO23MmtA1QHv9QcpqWzhc08zBU80cqnazt7KJN/dWRXpB2cwmpmYnM3OCESCKclzMnJDClAxnQlfx9dhGENlBxIFxsm7vaP8B8GullDfGZYuISRvBrj/BK3fDPR/CxPlRHxbyBqj69+0kzRlH5s0zh7ZMwyTk8ePZU4enpAZfxRkAbAWpOItzcM7LGvRcCpo2lrT6ghypcXOwupnD1c0cqjaCxMnG1sg+DquJopwUisJVTDPHG+uT05Nielc+6MbiRBKTQPDqN+HgW/D9T8EU/a1c84cnaXqjjJxvLcSWe27XuJEmUO/Fs6sGT0kNgZpWMAuOmZk4i7NJmjUu4au+NC1RNXv9HK4JVy2dcnO4xggS1WfaIvu47Bam57i6tEHMnJBCTop9SALEoBuLReQFpdRNIrKHbvr6K6Wiv4xONEoZA8kKP9OvIKBCCveWSmz5qaMiCABYMh2kXppHyiVT8Fe24CmpwbOrBu++04jdTNK8LJzFOdgL03Qjs6b1Q4rDyqK8DBblde2t1+TxcyhcvXQ4fPfw/v4aXthxIrJPqsNiBIYJKdx76XQmp8c2U0BvbQTfDj+ujmkJ4qG+DM6cgMLv9Osw7/56gvVe0q4qiE254khEsE12YZvsIu1zhbR92oinpIbW3XV4dlRjTrORFG5PsE3UmU41baDSnFYuKMjkgoLMLtvbey4dru6oZnpjdxXrLiuKeZl66z5aFX6MwUiuOItMS3lJvw5zbzqJOc1O0pzB5YhJdGISHEUZOIoyCH0+iHf/aTwltbg/PIl74wmsE5w4i3NIWpCDJX30Dm7StOGU5TLGMHTOQTVcVfe9VQ010336BwGUUmrk9nkr2wipk2Fc9LnhfVUttJU1kbaqADGPnSoSk81sdDddkEPQ7aM13Mjc9FYFTW9XYC9MM4LC3CxMSboPvKYNpeHq3t3bHcHoqAQ/WygEFR9C0ZXGePQouTedRKwmki8Y+vTBI4XZZcO1bBKuZZMI1LXiKa3BU1pLw0uHaXjtCEnnjcO5MBvHzExkFA220bTRLupLOBHJoesMZcdiUqJYq/kEPKf7NS1l0O3DU1pD8vnjddfKMEtWEqmX55OyMg//CXe4kbmW1j11SJIF53yjkdmWl6obmTUtwUUzQ9ka4OfAJIy5CPIxBpTNiW3RYqT8A+OxH/MPtGw7BQGVMFlGE4mIYJuSgm1KCmlXF+I90khrSQ2enTW0bDuFOd1ujE8ozsGa0/8Mr5qmxV40dwQ/AZYC7ymlikXkUuDW2BYrhso2wrjpkBbdSV0FQri3VmEvStcnsj6I2UTSzEySZmYSagvSuu80npIamjccp/lvx7FOduFcmE3S/GzMLtuYamvRtEQWTSDwK6VOi4hJRExKqb+JyE9jXrJYCPrh6CaYf3PUh7TurSPU7MO1NvZduEYTk91McnEOycU5BJt9eHbV4imtoemNcpreKDd2Eoy2BLMJsQhiMYWfh9fDj3Ra7/K6xTgucrw5/HqndbFI+PiOz4jsH66yUir8T6dHFTrr+Vmvo9RZx3W3rftjlQI6v39n0v7FdHouEtnc4z5IR5NXD+8R2dS+49nHmzCyrJnC3003j5h7ec00fI2b2tCKJhA0iogLI7XEsyJSg5E7aOSpLAGfG6ZG1z6glKL57yexZCXhmKFTOA+UOcVGyorJpKyYjL/Wg/dgA6otiAqEUMEQBJSxHgihggrCj5FtrQFjW6ftxnNlHB9UfRdCGx5C1yBhlt6DizkcoDptE4cFU1J4cYaXJGvH8yQLJqcVsZt1+9MQiSYQXAt4ge8AtwFpwL/GslAxU7YRECj4TFS7+4414z/hJv3aafoPbohYs51Ys4e2ik0pBcFugknnYNG+3v56MPwo4avYTlfNSPjquv25Sc7ar5t95Ox9zn6vHvYxEX6x052F8UOFH8ObOj0/+/XI0172Ub0db3xA5E5FhYzvExVeDylUCGNbqPM21WX/9ueEVMfvJLKNc4876/3at4Xqvfhb/YQ8AZT/3HkEIoRIwBCntWvwCAeLrgGl47nu1dZVb+MIfgn8USm1udPmp2JfpBgq3wgT5oEzs+99CXcZdZhxLkrMiWc0g4gY1UH6P/eoowIhQq0BQh5/+DHQ6fHsbX6Cp1uN9dZAr5Pgis1k3GU4O999WJFOzyNVi6ZO1ZRm4y5HzF2rMDGH9+m0jllGTFVZb3cEh4Gfi8hE4E/Ac0qp0uEpVgz4W+H4Nrjwnqh2DzS20bq3DtfyyZjsI3eGIk0bycRiwpxiw5xi69dxKqRQ3kAkKJwTODydX/Pjr2sl1NpMyOOHwBBWNZ4VHMQcbsMydVrvHDja19sDjElIuzIfc2psR/D3NqDsIeAhEckHbgGeCKekfg54Xil1KKYlG2rHtkLQF/X4gZatlaDAtXxSjAumadpQE5MY1UUDGPej/EFCrYEubVBnVylGqh9Dqmu7Vvv+QaMqkm63hY8LdH6/ELSFCAUDRhtY+/5BReqlU2LwDXXVZxtBONfQT4Gfikgx8DjwADCyLpPLN4LJAnnL+tw15AvSsv0Ujtnj9IQtmjbGiNWMeQTPUzwQfVaqiohVRK4RkWeBt4BDwNqYl2yolX8AuReA3dXnrp6SGkKeACkX6bsBTdNGv94ai6/AGDh2NbAdaJ/EvmWYyjZ0WhuNrqMXf7/PXZVSuDdXYp2YjK0wbRgKp2maFl+9VQ39EPgj8E9KqfphKk9sHN0EKhRV+0DbkUYC1R4ybpgxYlr8NU3TBqO3xuJLh7MgMVX+toPaxgAAEAZJREFUAViSILfPGdtwb6rElGzFuSB7GAqmaZoWf2Oj43XZRshfBpbeu2D561rxHqgn+cIJQzpXr8fjwe12D9n7aZqmDaXRP5OIuwZq98OCW/rctWVzJZgF19KhayQ+c+YMv/nNb/B4PDidTrKzs7ssOTk5JCcn62ooTdMIBoN4vV5aW1sjj/n5+dhs/RtH0V+jPxBEmXY65A3QsqMa5/xszKlD86UHg0FefPFF/H4/l19+OfX19dTW1rJnzx7a2toi+yUlJZ0THLKzs3G5XDpAaDHh8/loaGigoaGBxsZGGhoaCAQCWCwWrFbrgBazeWx1ueyJ3+8/52Tu9Xqj2ub3+895v3vvvZfs7NhWVY/+QFC2ARxpMHFBr7u1fFSN8gVxDWGX0Q0bNnDs2DGuu+46Fizo+HylFM3NzdTW1nZZPvnkE7xeb2Q/h8PRJTC0LykpKTpAhIVCIQKBAH6/v9+LUgqXy0VKSkpkcblcOByOEf/9BoNBzpw50+VE3/nE39LStfOf1WrFZrMRCATw+XwDmivXZDJhtVoHFUwsFgtmsxkRiekC9Pl6W1tb1CfwztuCwWCv35PNZsPhcOBwOEhKSiIjI4OkpKQu2zo/pqXFvvfi6A8E5R8YSeZMPV+tqJDCvaUSW34qttyhmaHz008/5cMPP6S4uLhLEADjDzA1NZXU1FSmTeuYN1kphdvt7hIcampq2LdvH62trZH92gPE2UtqamrCncDaTyxtbW1dHjufkAd6Ivf7/X3+p+uJ1WqMOO3uCsxisXQJDu0B4uxtdrs9bt+3UgqPx9PtSb6hoYGmpiZCoY6EbSJCWloaGRkZzJw5k4yMDNLT08nIyCAjIwOn09nlZwkGg/3+XfT2e/R4PD0G45Ho7JN2amrqOdu6O6k7HI6EvHMa3YGgoQIaj8Kyb/W6m3d/PcF6L2lXFQzJxzY3N/PSSy+RnZ3NqlWroj5ORCInmalTp0a2K6VoaWnpEhxqa2s5cOAAO3fujOxnt9u7bYPoT4BoP3F3d/Lubltvr/l8vn6fqHu6mrTZbCQnJ/d4Fdnfq87OV33Nzc2Rxe12d3leVVXFoUOHog4Y3QWNgQYMv9/f44m+oaEBn8/XZX+n00lGRgaTJ09m7ty5XU70qamp/ToBmc1mzGYzDkfsRtYrpQgGg+cEkFAohFIq7ovdbu/2pG632zGZRlc/m9EdCMo2Go99zD/g3nQSc5qdpDlZg/7IUCjESy+9hM/n44477hiSRh4RweVy4XK5KCws7PJae4BoDw61tbUcOnSIkpKSyD42m43s7GyysrIQkV5P3tGeuEUEu92OzWbr8uhyuc7ZZrPZzlm32WznnJwtFsuw/wez2+3Y7Xaysnr/3Q8mYFit1h7vKlwuFyaTqdsT/dk9zSwWS+TEnp+fH1lvv7q322ObmGyoiUjk9x7LgKP1bXQHgvIPwDUBsmb0uIuvqoW2siZSryoYkqkTN27cSEVFBddeey05OTmDfr++JCcnk5ycTEFBQZftne8g2peysrJzTuDJycl9nri729b5qnosGI6AAZCWlkZ6ejrTp08/50SvOw9osTK6A0HBCpg4v2Nqvm64N51ErCZcSyYM+uPKysrYuHEj8+fPZ+HChYN+v8HoKUBosTWQgBEKhUhPTyctLQ2LZXT/l9QS0+j+q1t8Z68vB90+PKU1JJ8/fkDpajtzu928/PLLZGVlcfXVV+srN61X0QYMTRsOMa2QFZHviMgnIrJXRJ4TEYeIPCsiB8PbHheRwZ2BB6Fl2ykIqEHPORAKhXj55Zfxer3ceOONI66uVtO0sS1mgUBEJgPrgMVKqbkY8xfcAjwLzALmAUnAV2NVht6oQAj31irsRelYxycP6r0+/PBDysrKWLVqFePH62ktNU0bWWLdRcMCJImIBXAClUqpN1UYRnrr3BiXoVute+sINftwXTR5UO9TUVHBhg0bmDt3LosWLRqi0mmapg2fmAUCpdRJ4D+BY0AV0KSUeqf99XCV0JeAt2NVhl7KRvPfT2LJSsIxI2PA79PS0sJLL71ERkYG11xzjW4X0DRtRIpl1VAGcC1QCEwCkkXki512+RXwgVLqwx6Ov1tEdojIjtra2iEtm+9YM/4TblzLJyGmgZ28Q6EQr7zyCh6PR7cLaJo2osWyauhyoFwpVauU8gMvA8sBROQBIBv4bk8HK6UeU0otVkotHuqES+5NJxGHGef5A6/P37x5M0eOHOGqq65i4sSJQ1g6TdO04RXLQHAMWCoiTjHqTFYC+0Xkq8A/ALcqpUK9vkMMBJraaN1bR/LiCZjsA8v5cezYMd5//31mz57N4sV9T3ajaZqWyGI2jkAptU1EXgR2AgGgBHgMaAGOAlvCdeovK6X+NVblOFvLlkpQDLjLqMfj4cUXXyQ9PZ01a9bodgFN00a8mA4oU0o9ADwwnJ/Zm5AvSMv2Uzhmj8OS2f/cJkopXn31VVpaWrjrrrt0fhRN00aF0ZVCrw+e0hpCngApA5xzYMuWLRw6dIgrr7ySSZOGbt4CTdO0eBozgUAphXtTJdaJydgK+z/Rw/Hjx3nvvfc477zzWLJkSQxKqGmaFh9jJhC0HWkkUO3BddGkftfrt7a28uKLL5KamqrbBTRNG3XGTCBwb6rElGzFuaB/qaHb2wWam5u54YYbSEpKilEJNU3T4mNMBAJ/XSveg/UkXzgBsfbvR962bRsHDx7kiiuuIDc3LtkwNE3TYmpMBIKWzZVgElxL+9fAe/LkSd555x1mzJjB0qVLY1Q6TdO0+Br1gSDkDdCyoxrnvCzMqdFPG9na2sr69etxuVx8/vOf1+0CmqaNWqM+ELTsqEb5grhWRJ9lVCnFn//8Z5qamrjhhhtwOp0xLKGmaVp8jepAoEIK9+ZKbPmp2HJToj7uo48+Yv/+/axcuZK8vLwYllDTNC3+RnUg8O6vJ1jvxdWPAWRVVVX89a9/Zfr06SxfvjyGpdM0TUsMozoQuDedxJxmJ2lOdPPCer1e1q9fj9Pp5LrrrsNkGtVfj6ZpGjDKJ69PXzONYFMbYu67oVcpxV/+8hcaGhq44447SE4e3PSVmqZpI8WoDgTWCclYJ0R3Qv/444/Zu3cvl112Gfn5+TEumaZpWuLQdR/AqVOnePvtt5k2bRorVqyId3E0TdOG1ZgPBG1tbaxfvx6Hw6HbBTRNG5PG9FlPKcUbb7xBfX09a9euxeVyxbtImqZpw25MB4KSkhJ2797NZz/7WQoLC+NdHE3TtLgYs4GgpqaGN998k8LCQi6++OJ4F0fTNC1uxmQg8Pl8vPDCC9jtdq6//nrdLqBp2pg2Js+Ab775JnV1daxdu5aUlOhTT2iapo1GYy4QlJaWUlpaysUXX8zUqVPjXRxN07S4G1OBoLa2ljfeeIP8/HwuueSSeBdH0zQtIYyZQOD3+1m/fj1Wq5W1a9fqdgFN07SwMXM2fOutt6ipqeH6668nNTU13sXRNE1LGGMiEOzZs4edO3eyYsUKpk+fHu/iaJqmJZRRHwjq6up4/fXXmTJlCpdeemm8i6NpmpZwRnUgaG8XMJvN3HDDDZjN5ngXSdM0LeGM6kDw17/+lerqaq677jrS0tLiXRxN07SENGoDgVKKzMxMVqxYwYwZM+JdHE3TtIQ1aiemERE957CmaVoUYnpHICLfEZFPRGSviDwnIg4R+ZaIHBERJSLRTSasaZqmxUzMAoGITAbWAYuVUnMBM3ALsAm4HDgaq8/WNE3TohfrqiELkCQifsAJVCqlSsCoutE0TdPiL2Z3BEqpk8B/AseAKqBJKfVOrD5P0zRNG5hYVg1lANcChcAkIFlEvtiP4+8WkR0isqO2tjZWxdQ0TRvzYtlYfDlQrpSqVUr5gZeBqLvxKKUeU0otVkotzs7OjlkhNU3TxrpYBoJjwFIRcYrRILAS2B/Dz9M0TdMGIJZtBNuAF4GdwJ7wZz0mIutE5ASQC+wWkd/Fqgyapmla30QpFe8y9ElEahn53U2zgLp4FyJB6O+iK/19dKW/jw6D/S7ylVJ91q2PiEAwGojIDqXU4niXIxHo76Ir/X10pb+PDsP1XYzaXEOapmladHQg0DRNG+N0IBg+j8W7AAlEfxdd6e+jK/19dBiW70K3EWiapo1x+o5A0zRtjNOBIIZEZIqI/E1E9ofTcX873mVKBCJiFpESEflLvMsSbyKSLiIvisiB8N/JsniXKV66S1sf7zINJxF5XERqRGRvp22ZIvKuiBwOP2bE4rN1IIitAPA9pdR5wFLgXhGZHecyJYJvo0eZt3sIeFspNQtYwBj9XnpJWz+WPAlcdda2+4H3lVJFwPvh50NOB4IYUkpVKaV2htebMf6TT45vqeJLRHKBq4ExP6JcRFKBi4HfAyilfEqpxviWKq7a09ZbCKetj3N5hpVS6gOg/qzN1wJPhdefAj4fi8/WgWCYiEgBUAxsi29J4u6/gP8FhOJdkAQwFagFnghXlf1ORJLjXah40GnrezReKVUFxoUlkBOLD9GBYBiIiAt4CfhHpdSZeJcnXkRkNVCjlPo43mVJEBZgEfBrpVQx0EKMbv0T3WDT1muDowNBjImIFSMIPKuUejne5Ymzi4A1IlIBPA9cJiLPxLdIcXUCOBFO0AhGksZFcSxPPA0qbf0oVi0iEwHCjzWx+BAdCGIonH7798B+pdQv4l2eeFNK/UAplauUKsBoCPwfpdSYvepTSp0CjovIzPCmlcC+OBYpnnTa+u79Gbg9vH478FosPiTWcxaPdRcBXwL2iEhpeNsPlVJvxrFMWmK5D3hWRGxAGXBnnMsTF0qpbSLSnrY+AJQwxkYYi8hzwCVAVjhV/wPAg8ALInIXRrC8MSafrUcWa5qmjW26akjTNG2M04FA0zRtjNOBQNM0bYzTgUDTNG2M04FA0zRtjNOBQNMAEQmKSGmnZchG+IpIQeeMkpqWaPQ4Ak0ztCqlFsa7EJoWD/qOQNN6ISIVIvJTEdkeXqaHt+eLyPsisjv8mBfePl5EXhGRXeGlPU2CWUT+O5xv/x0RSYrbD6VpZ9GBQNMMSWdVDd3c6bUzSqklwC8xsqcSXn9aKTUfeBZ4OLz9YWCjUmoBRt6gT8Lbi4BHlVJzgEZgbYx/Hk2Lmh5ZrGmAiLiVUq5utlcAlymlysIJBE8ppcaJSB0wUSnlD2+vUkpliUgtkKuUauv0HgXAu+HJRRCRfwasSql/i/1Ppml903cEmtY31cN6T/t0p63TehDdPqclEB0INK1vN3d63BJe30zHVIq3AX8Pr78PfAMiczOnDlchNW2g9FWJphmSOmWIBWMe4fYupHYR2YZx4XRreNs64HER+T7GLGPtWUO/DTwWzhYZxAgKVTEvvaYNgm4j0LRehNsIFiul6uJdFk2LFV01pGmaNsbpOwJN07QxTt8RaJqmjXE6EGiapo1xOhBomqaNcToQaJqmjXE6EGiapo1xOhBomqaNcf8fI/4m4d1Fz28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "xx=[1,2,3,4,5,6,7,8,9,10]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epa1,label='10k')\n",
    "plt.plot(xx,epa2,label='30k')\n",
    "plt.plot(xx,epa3,label='50k')\n",
    "plt.plot(xx,epa4,label='70k')\n",
    "plt.plot(xx,epa5,label='100k')\n",
    "plt.plot(xx,epa6,label='150k')\n",
    "plt.plot(xx,epa7,label='250k')\n",
    "plt.plot(xx,epa8,label='500k')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "xx=[1,2,3,4,5,6,7,8,9,10]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epl1,label='10k')\n",
    "plt.plot(xx,epl2,label='30k')\n",
    "plt.plot(xx,epl3,label='50k')\n",
    "plt.plot(xx,epl4,label='70k')\n",
    "plt.plot(xx,epl5,label='100k')\n",
    "plt.plot(xx,epl6,label='150k')\n",
    "plt.plot(xx,epl7,label='250k')\n",
    "plt.plot(xx,epl8,label='500k')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
