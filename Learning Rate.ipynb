{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=!ls aclImdb/train/pos\n",
    "train_neg=!ls aclImdb/train/neg\n",
    "test_pos=!ls aclImdb/test/pos\n",
    "test_neg=!ls aclImdb/test/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_targets=[]\n",
    "for i in range(0,len(train_pos)):\n",
    "    with open (\"aclImdb/train/pos/\"+train_pos[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(1))\n",
    "for i in range(0,len(train_neg)):\n",
    "    with open (\"aclImdb/train/neg/\"+train_neg[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_targets=[]\n",
    "for i in range(0,len(test_pos)):\n",
    "    with open (\"aclImdb/test/pos/\"+test_pos[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(1))\n",
    "for i in range(0,len(test_neg)):\n",
    "    with open (\"aclImdb/test/neg/\"+test_neg[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data[10000:12500] + train_data[22500:25000]\n",
    "val_targets=train_targets[10000:12500] + train_targets[22500:25000]\n",
    "\n",
    "train_data = train_data[0:10000] + train_data[12500:22500]\n",
    "train_targets =  train_targets[0:10000] + train_targets[12500:22500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_data.copy()\n",
    "y=test_data.copy()\n",
    "z=val_data.copy()\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "val_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(x)):\n",
    "    train_data.append(x[i][0])\n",
    "for i in range(0,len(y)):\n",
    "    test_data.append(y[i][0])\n",
    "for i in range(0,len(z)):\n",
    "    val_data.append(z[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords={'\\n','\\t','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', \n",
    "           'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an',\n",
    "           'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself',\n",
    "           'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "           'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n",
    "           'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "           'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all',\n",
    "           'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "           'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', \n",
    "           'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "           'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few',\n",
    "           'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it',\n",
    "           'how', 'further', 'was', 'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'looking', 'buying', 'u.k.', 'startup', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    ty= [token for token in u if (token not in stopwords)]\n",
    "    return ty\n",
    "\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking', 'looking buying', 'buying u.k.', 'u.k. startup', 'startup 1', '1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize2(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-1):\n",
    "        t.append(temp[i]+ ' '+temp[i+1])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize2(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying', 'looking buying u.k.', 'buying u.k. startup', 'u.k. startup 1', 'startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize3(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-2):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize3(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying u.k.', 'looking buying u.k. startup', 'buying u.k. startup 1', 'u.k. startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize4(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-3):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2]+' '+temp[i+3])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize4(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "#Function to tokenize food dataset. \n",
    "#Goes through every doc in dataset and converts to tokens.  Takes 15-20 minutes\n",
    "#Split documents in parallel and then tokenize.\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    token_dataset2 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens2 = []    \n",
    "    token_dataset3 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens3 = []    \n",
    "    token_dataset4 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens4 = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        tokens2 = tokenize2(sample)\n",
    "        tokens3 = tokenize3(sample)\n",
    "        tokens4 = tokenize4(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        token_dataset2.append(tokens2)\n",
    "        all_tokens2 += tokens2\n",
    "        token_dataset3.append(tokens3)\n",
    "        all_tokens3 += tokens3\n",
    "        token_dataset4.append(tokens4)\n",
    "        all_tokens4 += tokens4\n",
    "    return token_dataset, all_tokens,token_dataset2, all_tokens2,token_dataset3, all_tokens3,token_dataset4, all_tokens4\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5181552\n"
     ]
    }
   ],
   "source": [
    "#For bi-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"nntest_bi_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nnval_bi_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"nntrain_bi_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"nntrain_bi_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 100000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 23594 ; token mating\n",
      "Token mating; token id 23594\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 81.08\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 83.78\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 86.24\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 86.68\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 85.5\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 86.56\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 85.58\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 85.68\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 83.34\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 80.84\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 84.58\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 80.8\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 84.2\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 83.88\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 84.46\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 83.6\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 83.78\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 83.42\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa1=pa\n",
    "pl1=pl\n",
    "epa1=epa\n",
    "epl1=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 77.62\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 80.94\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 80.46\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 79.8\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 80.2\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 81.18\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 77.98\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 80.94\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 79.48\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 80.98\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 80.48\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 79.04\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 80.04\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 78.84\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 79.94\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 79.28\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 75.72\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 79.96\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "pa2=pa\n",
    "pl2=pl\n",
    "epa2=epa\n",
    "epl2=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 80.38\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 80.28\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 80.44\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 80.46\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 80.48\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 80.38\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 80.46\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 80.24\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 80.46\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 80.44\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 80.72\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 80.78\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 80.62\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 80.6\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 80.8\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 80.26\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 80.64\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 80.38\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "pa3=pa\n",
    "pl3=pl\n",
    "epa3=epa\n",
    "epl3=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 75.78\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 79.18\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 68.98\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 75.5\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 79.12\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 78.76\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 80.92\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 81.72\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 77.64\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 78.38\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 81.46\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 81.22\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 83.0\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 76.16\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 79.18\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 82.28\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 78.98\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 80.74\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "pa4=pa\n",
    "pl4=pl\n",
    "epa4=epa\n",
    "epl4=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx=[1,2,3]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epa1,label='0.01')\n",
    "plt.plot(xx,epa2,label='0.1')\n",
    "plt.plot(xx,epa3,label='0.001')\n",
    "plt.plot(xx,epa4,label='1')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx=[1,2,3]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epl1,label='0.01')\n",
    "plt.plot(xx,epl2,label='0.1')\n",
    "plt.plot(xx,epl3,label='0.001')\n",
    "plt.plot(xx,epl4,label='1')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
