{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by downloading 20-newsgroup text dataset:\n",
    "\n",
    "```http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=!ls aclImdb/train/pos\n",
    "train_neg=!ls aclImdb/train/neg\n",
    "test_pos=!ls aclImdb/test/pos\n",
    "test_neg=!ls aclImdb/test/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_targets=[]\n",
    "for i in range(0,len(train_pos)):\n",
    "    with open (\"aclImdb/train/pos/\"+train_pos[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(1))\n",
    "for i in range(0,len(train_neg)):\n",
    "    with open (\"aclImdb/train/neg/\"+train_neg[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_targets=[]\n",
    "for i in range(0,len(test_pos)):\n",
    "    with open (\"aclImdb/test/pos/\"+test_pos[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(1))\n",
    "for i in range(0,len(test_neg)):\n",
    "    with open (\"aclImdb/test/neg/\"+test_neg[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data[10000:12500] + train_data[22500:25000]\n",
    "val_targets=train_targets[10000:12500] + train_targets[22500:25000]\n",
    "\n",
    "train_data = train_data[0:10000] + train_data[12500:22500]\n",
    "train_targets =  train_targets[0:10000] + train_targets[12500:22500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_data.copy()\n",
    "y=test_data.copy()\n",
    "z=val_data.copy()\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "val_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(x)):\n",
    "    train_data.append(x[i][0])\n",
    "for i in range(0,len(y)):\n",
    "    test_data.append(y[i][0])\n",
    "for i in range(0,len(z)):\n",
    "    val_data.append(z[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens] #if (token.text not in punctuations)]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple is', 'is looking', 'looking at', 'at buying', 'buying u.k.', 'u.k. startup', 'startup for', 'for $', '$ 1', '1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize2(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    temp= [token.text.lower() for token in tokens] #if (token.text not in punctuations)]\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-1):\n",
    "        t.append(temp[i]+ ' '+temp[i+1])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize2(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple is looking', 'is looking at', 'looking at buying', 'at buying u.k.', 'buying u.k. startup', 'u.k. startup for', 'startup for $', 'for $ 1', '$ 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize3(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    temp= [token.text.lower() for token in tokens] #if (token.text not in punctuations)]\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-2):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize3(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple is looking at', 'is looking at buying', 'looking at buying u.k.', 'at buying u.k. startup', 'buying u.k. startup for', 'u.k. startup for $', 'startup for $ 1', 'for $ 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize4(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    temp= [token.text.lower() for token in tokens] #if (token.text not in punctuations)]\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-3):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2]+' '+temp[i+3])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize4(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "#Function to tokenize food dataset. \n",
    "#Goes through every doc in dataset and converts to tokens.  Takes 15-20 minutes\n",
    "#Split documents in parallel and then tokenize.\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    token_dataset2 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens2 = []    \n",
    "    token_dataset3 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens3 = []    \n",
    "    token_dataset4 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens4 = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        tokens2 = tokenize2(sample)\n",
    "        tokens3 = tokenize3(sample)\n",
    "        tokens4 = tokenize4(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        token_dataset2.append(tokens2)\n",
    "        all_tokens2 += tokens2\n",
    "        token_dataset3.append(tokens3)\n",
    "        all_tokens3 += tokens3\n",
    "        token_dataset4.append(tokens4)\n",
    "        all_tokens4 += tokens4\n",
    "    return token_dataset, all_tokens,token_dataset2, all_tokens2,token_dataset3, all_tokens3,token_dataset4, all_tokens4\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "a,b,c,d,e,f,g,h = tokenize_dataset(val_data)\n",
    "pkl.dump(a, open(\"nval_uni_tokens.p\", \"wb\"))\n",
    "pkl.dump(b, open(\"nval_uni_alltokens.p\", \"wb\"))\n",
    "pkl.dump(c, open(\"nval_bi_tokens.p\", \"wb\"))\n",
    "pkl.dump(d, open(\"nval_bi_alltokens.p\", \"wb\"))\n",
    "pkl.dump(e, open(\"nval_tri_tokens.p\", \"wb\"))\n",
    "pkl.dump(f, open(\"nval_tri_alltokens.p\", \"wb\"))\n",
    "pkl.dump(g, open(\"nval_quad_tokens.p\", \"wb\"))\n",
    "pkl.dump(h, open(\"nval_quad_alltokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "a,b,c,d,e,f,g,h = tokenize_dataset(test_data)\n",
    "pkl.dump(a, open(\"ntest_uni_tokens.p\", \"wb\"))\n",
    "pkl.dump(b, open(\"ntest_uni_alltokens.p\", \"wb\"))\n",
    "pkl.dump(c, open(\"ntest_bi_tokens.p\", \"wb\"))\n",
    "pkl.dump(d, open(\"ntest_bi_alltokens.p\", \"wb\"))\n",
    "pkl.dump(e, open(\"ntest_tri_tokens.p\", \"wb\"))\n",
    "pkl.dump(f, open(\"ntest_tri_alltokens.p\", \"wb\"))\n",
    "pkl.dump(g, open(\"ntest_quad_tokens.p\", \"wb\"))\n",
    "pkl.dump(h, open(\"ntest_quad_alltokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "a,b,c,d,e,f,g,h = tokenize_dataset(train_data)\n",
    "pkl.dump(a, open(\"ntrain_uni_tokens.p\", \"wb\"))\n",
    "pkl.dump(b, open(\"ntrain_uni_alltokens.p\", \"wb\"))\n",
    "pkl.dump(c, open(\"ntrain_bi_tokens.p\", \"wb\"))\n",
    "pkl.dump(d, open(\"ntrain_bi_alltokens.p\", \"wb\"))\n",
    "pkl.dump(e, open(\"ntrain_tri_tokens.p\", \"wb\"))\n",
    "pkl.dump(f, open(\"ntrain_tri_alltokens.p\", \"wb\"))\n",
    "pkl.dump(g, open(\"ntrain_quad_tokens.p\", \"wb\"))\n",
    "pkl.dump(h, open(\"ntrain_quad_alltokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(train_targets, open(\"ntrain_targets.p\", \"wb\"))\n",
    "pkl.dump(test_targets, open(\"ntest_targets.p\", \"wb\"))\n",
    "pkl.dump(val_targets, open(\"nval_targets.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5439615\n"
     ]
    }
   ],
   "source": [
    "#For uni-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"ntest_uni_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nval_uni_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"ntrain_uni_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"ntrain_uni_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = val_uni_tokens\n",
    "test_data_tokens = test_uni_tokens\n",
    "train_data_tokens = train_uni_tokens\n",
    "all_train_tokens = all_train_uni \n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2964 ; token replaced\n",
      "Token replaced; token id 2964\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 76.68\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.86\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 82.18\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 81.66\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 82.58\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 80.24\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 83.66\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 84.08\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 83.62\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 82.44\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 83.0\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 83.12\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 83.3\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 82.98\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 81.92\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 82.24\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 81.62\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 81.46\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 81.86\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 81.36\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 80.98\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 80.9\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 80.98\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 81.6\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 81.28\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 81.24\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 80.56\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 80.38\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 80.44\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 80.94\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 80.96\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 80.7\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 80.58\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 80.74\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 80.36\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 80.12\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 80.64\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 79.74\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 80.14\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 79.42\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 79.5\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 79.7\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 79.88\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 79.28\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 79.8\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 79.8\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 79.24\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 79.54\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 79.82\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 79.42\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 79.28\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 79.48\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 79.42\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 79.5\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 79.16\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 79.6\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 79.5\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 79.68\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 79.38\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 79.08\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl1=pl\n",
    "pa1=pa\n",
    "epl1=epl\n",
    "epa1=epa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 78.24\n",
      "Test Acc 79.548\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 10859230\n"
     ]
    }
   ],
   "source": [
    "#For bi-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"ntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"ntest_bi_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nval_bi_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"ntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"ntrain_bi_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"ntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"ntrain_bi_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 723 ; token , to\n",
      "Token , to; token id 723\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 68.12\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.68\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 82.42\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 81.72\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 83.54\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 83.86\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 84.54\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 84.12\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 83.98\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.12\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 83.82\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 84.24\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.06\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 83.42\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 84.0\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 83.24\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 83.3\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.26\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 82.98\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 82.9\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 82.94\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 82.34\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.32\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 82.12\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 82.28\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 81.76\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.0\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 82.48\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 81.54\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 81.42\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 81.54\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 81.58\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 81.26\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 81.0\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 81.1\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 81.98\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 81.58\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 81.2\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 81.18\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 80.42\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 81.26\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 80.12\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 81.2\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 81.0\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 81.38\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 79.36\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 78.84\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 81.16\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 80.86\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 80.2\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 78.62\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 80.52\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 80.62\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 80.32\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 80.58\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 80.26\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 79.6\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 79.92\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 80.44\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 80.46\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa2=pa\n",
    "pl2=pl\n",
    "epa2=epa\n",
    "epl2=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 80.42\n",
      "Test Acc 80.68\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 16258845\n"
     ]
    }
   ],
   "source": [
    "#For tri-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"ntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"ntest_bi_tokens.p\", \"rb\"))\n",
    "test_tri_tokens = pkl.load(open(\"ntest_tri_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nval_bi_tokens.p\", \"rb\"))\n",
    "val_tri_tokens = pkl.load(open(\"nval_tri_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"ntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"ntrain_bi_tokens.p\", \"rb\"))\n",
    "train_tri_tokens = pkl.load(open(\"ntrain_tri_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"ntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"ntrain_bi_alltokens.p\", \"rb\"))\n",
    "all_train_tri = pkl.load(open(\"ntrain_tri_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi + all_train_tri\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i] + test_tri_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i] + train_tri_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i] + val_tri_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 3964 ; token criminal\n",
      "Token criminal; token id 3964\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 76.14\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.14\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 82.18\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 83.36\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 83.74\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 84.06\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 84.48\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 84.32\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 83.22\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 83.84\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 83.96\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 83.02\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.0\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 83.72\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 84.22\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 83.68\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 82.6\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 82.8\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 83.4\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 82.58\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 82.08\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 83.24\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.74\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 83.2\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 82.74\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 82.48\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.26\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 82.54\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 81.94\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.02\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 81.2\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 82.12\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 82.06\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 82.36\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 81.7\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 81.96\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 81.74\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 81.76\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 81.42\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 81.98\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 81.46\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 81.66\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 81.54\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 81.56\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 81.12\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 81.44\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 79.3\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 81.8\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 81.64\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 79.04\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 81.26\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 81.28\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 81.12\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 81.46\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 81.18\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 81.0\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 80.86\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 81.26\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 81.04\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 81.3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa3=pa\n",
    "pl3=pl\n",
    "epa3=epa\n",
    "epl3=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 80.38\n",
      "Test Acc 80.332\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 21638460\n"
     ]
    }
   ],
   "source": [
    "#For quad-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"ntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"ntest_bi_tokens.p\", \"rb\"))\n",
    "test_tri_tokens = pkl.load(open(\"ntest_tri_tokens.p\", \"rb\"))\n",
    "test_quad_tokens = pkl.load(open(\"ntest_quad_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nval_bi_tokens.p\", \"rb\"))\n",
    "val_tri_tokens = pkl.load(open(\"nval_tri_tokens.p\", \"rb\"))\n",
    "val_quad_tokens = pkl.load(open(\"nval_quad_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"ntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"ntrain_bi_tokens.p\", \"rb\"))\n",
    "train_tri_tokens = pkl.load(open(\"ntrain_tri_tokens.p\", \"rb\"))\n",
    "train_quad_tokens = pkl.load(open(\"ntrain_quad_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"ntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"ntrain_bi_alltokens.p\", \"rb\"))\n",
    "all_train_tri = pkl.load(open(\"ntrain_tri_alltokens.p\", \"rb\"))\n",
    "all_train_quad = pkl.load(open(\"ntrain_quad_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi + all_train_tri + all_train_quad \n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i] + test_tri_tokens[i]+ test_quad_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i] + train_tri_tokens[i]+ train_quad_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i] + val_tri_tokens[i]+ val_quad_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 9386 ; token . again\n",
      "Token . again; token id 9386\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 80.7\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 80.14\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 80.7\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 80.46\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 80.72\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 81.02\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 80.5\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 80.74\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 81.0\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 80.26\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 80.82\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 80.5\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 80.86\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 80.64\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 80.84\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 80.68\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 80.6\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 79.8\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 80.26\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 80.24\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 79.72\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 80.04\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 79.74\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 79.02\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 79.78\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 79.64\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 80.48\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 78.64\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 80.48\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 80.0\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 80.32\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 80.02\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 79.52\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 80.0\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 80.12\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 79.88\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 79.78\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 79.8\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 79.88\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 79.88\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 79.54\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 79.8\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 79.78\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 79.3\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 79.22\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 79.18\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 79.7\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 79.54\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 79.32\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 79.42\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 79.38\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 78.76\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 79.46\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 79.32\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 78.46\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 79.3\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 79.24\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 79.4\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 78.82\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 78.92\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa4=pa\n",
    "pl4=pl\n",
    "epa4=epa\n",
    "epl4=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 78.54\n",
      "Test Acc 78.636\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.74366666666667"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa1)/len(epa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.56800000000001"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa2)/len(epa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.10533333333333"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa3)/len(epa3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.90133333333333"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa4)/len(epa4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For Uni-gram, average validation loss and average validation accuracy PER \n",
    "xx=[1,2,3,4,5,6,7,8,9,10]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epl1,'r',label='uni')\n",
    "plt.plot(xx,epl2,'b',label='bi')\n",
    "plt.plot(xx,epl3,'g',label='tri')\n",
    "plt.plot(xx,epl4,'y',label='quad')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For Uni-gram, average validation loss and average validation accuracy PER \n",
    "xx=[1,2,3,4,5,6,7,8,9,10]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epa1,'r',label='uni')\n",
    "plt.plot(xx,epa2,'b',label='bi')\n",
    "plt.plot(xx,epa3,'g',label='tri')\n",
    "plt.plot(xx,epa4,'y',label='quad')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
