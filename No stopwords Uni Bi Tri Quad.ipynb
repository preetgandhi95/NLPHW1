{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by downloading 20-newsgroup text dataset:\n",
    "\n",
    "```http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=!ls aclImdb/train/pos\n",
    "train_neg=!ls aclImdb/train/neg\n",
    "test_pos=!ls aclImdb/test/pos\n",
    "test_neg=!ls aclImdb/test/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_targets=[]\n",
    "for i in range(0,len(train_pos)):\n",
    "    with open (\"aclImdb/train/pos/\"+train_pos[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(1))\n",
    "for i in range(0,len(train_neg)):\n",
    "    with open (\"aclImdb/train/neg/\"+train_neg[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_targets=[]\n",
    "for i in range(0,len(test_pos)):\n",
    "    with open (\"aclImdb/test/pos/\"+test_pos[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(1))\n",
    "for i in range(0,len(test_neg)):\n",
    "    with open (\"aclImdb/test/neg/\"+test_neg[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data[10000:12500] + train_data[22500:25000]\n",
    "val_targets=train_targets[10000:12500] + train_targets[22500:25000]\n",
    "\n",
    "train_data = train_data[0:10000] + train_data[12500:22500]\n",
    "train_targets =  train_targets[0:10000] + train_targets[12500:22500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_data.copy()\n",
    "y=test_data.copy()\n",
    "z=val_data.copy()\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "val_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(x)):\n",
    "    train_data.append(x[i][0])\n",
    "for i in range(0,len(y)):\n",
    "    test_data.append(y[i][0])\n",
    "for i in range(0,len(z)):\n",
    "    val_data.append(z[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords={'\\n','\\t','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', \n",
    "           'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an',\n",
    "           'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself',\n",
    "           'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "           'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n",
    "           'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "           'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all',\n",
    "           'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "           'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', \n",
    "           'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "           'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few',\n",
    "           'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it',\n",
    "           'how', 'further', 'was', 'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'looking', 'buying', 'u.k.', 'startup', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    ty= [token for token in u if (token not in stopwords)]\n",
    "    return ty\n",
    "\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking', 'looking buying', 'buying u.k.', 'u.k. startup', 'startup 1', '1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize2(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-1):\n",
    "        t.append(temp[i]+ ' '+temp[i+1])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize2(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying', 'looking buying u.k.', 'buying u.k. startup', 'u.k. startup 1', 'startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize3(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-2):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize3(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying u.k.', 'looking buying u.k. startup', 'buying u.k. startup 1', 'u.k. startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize4(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-3):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2]+' '+temp[i+3])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize4(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "#Function to tokenize food dataset. \n",
    "#Goes through every doc in dataset and converts to tokens.  Takes 15-20 minutes\n",
    "#Split documents in parallel and then tokenize.\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    token_dataset2 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens2 = []    \n",
    "    token_dataset3 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens3 = []    \n",
    "    token_dataset4 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens4 = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        tokens2 = tokenize2(sample)\n",
    "        tokens3 = tokenize3(sample)\n",
    "        tokens4 = tokenize4(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        token_dataset2.append(tokens2)\n",
    "        all_tokens2 += tokens2\n",
    "        token_dataset3.append(tokens3)\n",
    "        all_tokens3 += tokens3\n",
    "        token_dataset4.append(tokens4)\n",
    "        all_tokens4 += tokens4\n",
    "    return token_dataset, all_tokens,token_dataset2, all_tokens2,token_dataset3, all_tokens3,token_dataset4, all_tokens4\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "a,b,c,d,e,f,g,h = tokenize_dataset(val_data)\n",
    "pkl.dump(a, open(\"nnval_uni_tokens.p\", \"wb\"))\n",
    "pkl.dump(b, open(\"nnval_uni_alltokens.p\", \"wb\"))\n",
    "pkl.dump(c, open(\"nnval_bi_tokens.p\", \"wb\"))\n",
    "pkl.dump(d, open(\"nnval_bi_alltokens.p\", \"wb\"))\n",
    "pkl.dump(e, open(\"nnval_tri_tokens.p\", \"wb\"))\n",
    "pkl.dump(f, open(\"nnval_tri_alltokens.p\", \"wb\"))\n",
    "pkl.dump(g, open(\"nnval_quad_tokens.p\", \"wb\"))\n",
    "pkl.dump(h, open(\"nnval_quad_alltokens.p\", \"wb\"))\n",
    "#test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "a,b,c,d,e,f,g,h = tokenize_dataset(test_data)\n",
    "pkl.dump(a, open(\"nntest_uni_tokens.p\", \"wb\"))\n",
    "pkl.dump(b, open(\"nntest_uni_alltokens.p\", \"wb\"))\n",
    "pkl.dump(c, open(\"nntest_bi_tokens.p\", \"wb\"))\n",
    "pkl.dump(d, open(\"nntest_bi_alltokens.p\", \"wb\"))\n",
    "pkl.dump(e, open(\"nntest_tri_tokens.p\", \"wb\"))\n",
    "pkl.dump(f, open(\"nntest_tri_alltokens.p\", \"wb\"))\n",
    "pkl.dump(g, open(\"nntest_quad_tokens.p\", \"wb\"))\n",
    "pkl.dump(h, open(\"nntest_quad_alltokens.p\", \"wb\"))\n",
    "#train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "a,b,c,d,e,f,g,h = tokenize_dataset(train_data)\n",
    "pkl.dump(a, open(\"nntrain_uni_tokens.p\", \"wb\"))\n",
    "pkl.dump(b, open(\"nntrain_uni_alltokens.p\", \"wb\"))\n",
    "pkl.dump(c, open(\"nntrain_bi_tokens.p\", \"wb\"))\n",
    "pkl.dump(d, open(\"nntrain_bi_alltokens.p\", \"wb\"))\n",
    "pkl.dump(e, open(\"nntrain_tri_tokens.p\", \"wb\"))\n",
    "pkl.dump(f, open(\"nntrain_tri_alltokens.p\", \"wb\"))\n",
    "pkl.dump(g, open(\"nntrain_quad_tokens.p\", \"wb\"))\n",
    "pkl.dump(h, open(\"nntrain_quad_alltokens.p\", \"wb\"))\n",
    "pkl.dump(train_targets, open(\"nntrain_targets.p\", \"wb\"))\n",
    "pkl.dump(test_targets, open(\"nntest_targets.p\", \"wb\"))\n",
    "pkl.dump(val_targets, open(\"nnval_targets.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 2600776\n"
     ]
    }
   ],
   "source": [
    "#For uni-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = val_uni_tokens\n",
    "test_data_tokens = test_uni_tokens\n",
    "train_data_tokens = train_uni_tokens\n",
    "all_train_tokens = all_train_uni \n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 3282 ; token dr\n",
      "Token dr; token id 3282\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 81.7\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.56\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 83.02\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.44\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 86.66\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.78\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.18\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 83.86\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 84.24\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 83.9\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 84.32\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 85.16\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 84.22\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.54\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 83.84\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 84.02\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 84.08\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 83.06\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 83.2\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 82.98\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 80.62\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.16\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 82.66\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 81.9\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 82.44\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 82.9\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.4\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 82.58\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 83.06\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.78\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 82.22\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 82.32\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 82.9\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 82.18\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 81.72\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 81.8\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 81.98\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 82.2\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 82.04\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 82.26\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 81.9\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 81.6\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 81.74\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 81.58\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.02\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 81.78\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 81.58\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 81.62\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 81.66\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 81.5\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 81.28\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 80.48\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 81.16\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 81.26\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 81.14\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 81.42\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 80.82\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 81.26\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 81.9\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 81.3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl1=pl\n",
    "pa1=pa\n",
    "epl1=epl\n",
    "epa1=epa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 80.78\n",
      "Test Acc 81.324\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5181552\n"
     ]
    }
   ],
   "source": [
    "#For bi-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"nntest_bi_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nnval_bi_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"nntrain_bi_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"nntrain_bi_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5806 ; token pamela\n",
      "Token pamela; token id 5806\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 81.08\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.9\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 86.42\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.8\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 87.06\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 84.7\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.58\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.54\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 85.76\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 86.44\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 85.08\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 85.42\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 85.74\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 84.7\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 85.64\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 80.78\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 85.28\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 85.22\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 85.16\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.02\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.88\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.08\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.24\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.52\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 83.98\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.12\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 82.72\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 83.66\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 83.1\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 83.08\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.34\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.84\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.56\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 83.44\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.1\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.68\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 82.92\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 83.28\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 83.38\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 83.18\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.28\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 82.88\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.3\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 82.04\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.28\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 83.04\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 82.74\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 83.46\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 83.14\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.88\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 82.9\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.88\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.52\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 81.86\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 82.0\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.1\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.8\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 81.6\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.54\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa2=pa\n",
    "pl2=pl\n",
    "epa2=epa\n",
    "epl2=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 82.18\n",
      "Test Acc 82.6\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 7742328\n"
     ]
    }
   ],
   "source": [
    "#For tri-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"nntest_bi_tokens.p\", \"rb\"))\n",
    "test_tri_tokens = pkl.load(open(\"nntest_tri_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nnval_bi_tokens.p\", \"rb\"))\n",
    "val_tri_tokens = pkl.load(open(\"nnval_tri_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"nntrain_bi_tokens.p\", \"rb\"))\n",
    "train_tri_tokens = pkl.load(open(\"nntrain_tri_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"nntrain_bi_alltokens.p\", \"rb\"))\n",
    "all_train_tri = pkl.load(open(\"nntrain_tri_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi + all_train_tri\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i] + test_tri_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i] + train_tri_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i] + val_tri_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5357 ; token three times\n",
      "Token three times; token id 5357\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 80.24\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.16\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 84.56\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.48\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 87.08\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 87.02\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 87.06\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.22\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 83.66\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 84.86\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 84.38\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 86.32\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 86.06\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 85.24\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 85.76\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 85.94\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 85.02\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 84.82\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 84.98\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 85.04\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.76\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.88\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.44\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.1\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.24\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.24\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.12\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 83.18\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.04\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 80.7\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 83.4\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.32\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.8\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 83.62\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.58\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.88\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 83.2\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 82.36\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 83.72\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 82.04\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.5\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 83.38\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 83.04\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 83.08\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.8\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.34\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 82.98\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 82.68\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 83.08\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.06\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 83.26\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.76\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.44\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 83.06\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 83.1\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 83.16\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 83.2\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 80.98\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.3\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.6\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa3=pa\n",
    "pl3=pl\n",
    "epa3=epa\n",
    "epl3=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 82.24\n",
      "Test Acc 82.868\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 10283104\n"
     ]
    }
   ],
   "source": [
    "#For quad-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"nntest_bi_tokens.p\", \"rb\"))\n",
    "test_tri_tokens = pkl.load(open(\"nntest_tri_tokens.p\", \"rb\"))\n",
    "test_quad_tokens = pkl.load(open(\"nntest_quad_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nnval_bi_tokens.p\", \"rb\"))\n",
    "val_tri_tokens = pkl.load(open(\"nnval_tri_tokens.p\", \"rb\"))\n",
    "val_quad_tokens = pkl.load(open(\"nnval_quad_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"nntrain_bi_tokens.p\", \"rb\"))\n",
    "train_tri_tokens = pkl.load(open(\"nntrain_tri_tokens.p\", \"rb\"))\n",
    "train_quad_tokens = pkl.load(open(\"nntrain_quad_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"nntrain_bi_alltokens.p\", \"rb\"))\n",
    "all_train_tri = pkl.load(open(\"nntrain_tri_alltokens.p\", \"rb\"))\n",
    "all_train_quad = pkl.load(open(\"nntrain_quad_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi + all_train_tri + all_train_quad \n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i] + test_tri_tokens[i]+ test_quad_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i] + train_tri_tokens[i]+ train_quad_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i] + val_tri_tokens[i]+ val_quad_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 7985 ; token alvin\n",
      "Token alvin; token id 7985\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 67.36\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.06\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 83.56\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.4\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 86.68\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 83.52\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.62\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.14\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 86.68\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 86.66\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 86.12\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 86.6\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 86.06\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 86.1\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 86.04\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 84.34\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 85.42\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 85.8\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 85.2\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 83.86\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.9\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 85.12\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.72\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.56\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.34\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.7\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.4\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.76\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 83.88\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 82.74\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 84.06\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.5\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.76\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 83.88\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.0\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.24\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 82.34\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 82.72\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 83.64\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 83.66\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 83.68\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 82.4\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 83.34\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 83.76\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.36\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 83.48\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 83.34\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 83.46\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 83.26\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 82.48\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.8\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.28\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 82.58\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 83.22\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.48\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 83.46\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 83.1\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.8\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 83.06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa4=pa\n",
    "pl4=pl\n",
    "epa4=epa\n",
    "epl4=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 82.74\n",
      "Test Acc 82.868\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.71466666666666"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa1)/len(epa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.79933333333334"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa2)/len(epa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.872"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa3)/len(epa3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.83033333333333"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(epa4)/len(epa4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.86"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(epa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.96999999999998"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(epa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.47333333333331"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(epa3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.235542"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(epa4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For Uni-gram, average validation loss and average validation accuracy PER \n",
    "xx=[1,2,3,4,5,6,7,8,9,10]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epl1,'r',label='uni')\n",
    "plt.plot(xx,epl2,'b',label='bi')\n",
    "plt.plot(xx,epl3,'g',label='tri')\n",
    "plt.plot(xx,epl4,'y',label='quad')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For Uni-gram, average validation loss and average validation accuracy PER \n",
    "xx=[1,2,3,4,5,6,7,8,9,10]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epa1,'r',label='uni')\n",
    "plt.plot(xx,epa2,'b',label='bi')\n",
    "plt.plot(xx,epa3,'g',label='tri')\n",
    "plt.plot(xx,epa4,'y',label='quad')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
