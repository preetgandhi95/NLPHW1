{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=!ls aclImdb/train/pos\n",
    "train_neg=!ls aclImdb/train/neg\n",
    "test_pos=!ls aclImdb/test/pos\n",
    "test_neg=!ls aclImdb/test/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_targets=[]\n",
    "for i in range(0,len(train_pos)):\n",
    "    with open (\"aclImdb/train/pos/\"+train_pos[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(1))\n",
    "for i in range(0,len(train_neg)):\n",
    "    with open (\"aclImdb/train/neg/\"+train_neg[i], \"r\") as myfile:\n",
    "        train_data.append(myfile.readlines())\n",
    "        train_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "test_targets=[]\n",
    "for i in range(0,len(test_pos)):\n",
    "    with open (\"aclImdb/test/pos/\"+test_pos[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(1))\n",
    "for i in range(0,len(test_neg)):\n",
    "    with open (\"aclImdb/test/neg/\"+test_neg[i], \"r\") as myfile:\n",
    "        test_data.append(myfile.readlines())\n",
    "        test_targets.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data[10000:12500] + train_data[22500:25000]\n",
    "val_targets=train_targets[10000:12500] + train_targets[22500:25000]\n",
    "\n",
    "train_data = train_data[0:10000] + train_data[12500:22500]\n",
    "train_targets =  train_targets[0:10000] + train_targets[12500:22500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_data.copy()\n",
    "y=test_data.copy()\n",
    "z=val_data.copy()\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "val_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(x)):\n",
    "    train_data.append(x[i][0])\n",
    "for i in range(0,len(y)):\n",
    "    test_data.append(y[i][0])\n",
    "for i in range(0,len(z)):\n",
    "    val_data.append(z[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/preetgandhi95/miniconda3/envs/nlpclass/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords={'\\n','\\t','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', \n",
    "           'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an',\n",
    "           'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself',\n",
    "           'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "           'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n",
    "           'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "           'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all',\n",
    "           'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "           'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', \n",
    "           'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "           'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few',\n",
    "           'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it',\n",
    "           'how', 'further', 'was', 'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'looking', 'buying', 'u.k.', 'startup', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    ty= [token for token in u if (token not in stopwords)]\n",
    "    return ty\n",
    "\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking', 'looking buying', 'buying u.k.', 'u.k. startup', 'startup 1', '1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize2(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-1):\n",
    "        t.append(temp[i]+ ' '+temp[i+1])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize2(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying', 'looking buying u.k.', 'buying u.k. startup', 'u.k. startup 1', 'startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize3(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-2):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize3(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple looking buying u.k.', 'looking buying u.k. startup', 'buying u.k. startup 1', 'u.k. startup 1 billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize4(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    u= [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    temp= [token for token in u if (token not in stopwords)]\n",
    "\n",
    "    t=[]\n",
    "    for i in range(0,len(temp)-3):\n",
    "        t.append(temp[i]+ ' '+temp[i+1]+' '+temp[i+2]+' '+temp[i+3])\n",
    "    return t\n",
    "\n",
    "# Example\n",
    "tokens = tokenize4(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "#Function to tokenize food dataset. \n",
    "#Goes through every doc in dataset and converts to tokens.  Takes 15-20 minutes\n",
    "#Split documents in parallel and then tokenize.\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    token_dataset2 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens2 = []    \n",
    "    token_dataset3 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens3 = []    \n",
    "    token_dataset4 = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens4 = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        tokens2 = tokenize2(sample)\n",
    "        tokens3 = tokenize3(sample)\n",
    "        tokens4 = tokenize4(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        token_dataset2.append(tokens2)\n",
    "        all_tokens2 += tokens2\n",
    "        token_dataset3.append(tokens3)\n",
    "        all_tokens3 += tokens3\n",
    "        token_dataset4.append(tokens4)\n",
    "        all_tokens4 += tokens4\n",
    "    return token_dataset, all_tokens,token_dataset2, all_tokens2,token_dataset3, all_tokens3,token_dataset4, all_tokens4\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5181552\n"
     ]
    }
   ],
   "source": [
    "#For bi-grams\n",
    "import pickle as pkl\n",
    "# FOR TEST\n",
    "test_uni_tokens = pkl.load(open(\"nntest_uni_tokens.p\", \"rb\"))\n",
    "test_bi_tokens = pkl.load(open(\"nntest_bi_tokens.p\", \"rb\"))\n",
    "#FOR VAL\n",
    "val_uni_tokens = pkl.load(open(\"nnval_uni_tokens.p\", \"rb\"))\n",
    "val_bi_tokens = pkl.load(open(\"nnval_bi_tokens.p\", \"rb\"))\n",
    "#FOR TRAIN\n",
    "train_uni_tokens = pkl.load(open(\"nntrain_uni_tokens.p\", \"rb\"))\n",
    "train_bi_tokens = pkl.load(open(\"nntrain_bi_tokens.p\", \"rb\"))\n",
    "#ALL TOKENS\n",
    "all_train_uni = pkl.load(open(\"nntrain_uni_alltokens.p\", \"rb\"))\n",
    "all_train_bi = pkl.load(open(\"nntrain_bi_alltokens.p\", \"rb\"))\n",
    "#COMBINING\n",
    "val_data_tokens = []\n",
    "test_data_tokens = []\n",
    "train_data_tokens = []\n",
    "all_train_tokens = all_train_uni + all_train_bi\n",
    "for i in range(0,len(test_uni_tokens)):\n",
    "    test_data_tokens.append(test_uni_tokens[i] + test_bi_tokens[i])\n",
    "for i in range(0,len(train_uni_tokens)):\n",
    "    train_data_tokens.append(train_uni_tokens[i] + train_bi_tokens[i])\n",
    "for i in range(0,len(val_uni_tokens)):\n",
    "    val_data_tokens.append(val_uni_tokens[i] + val_bi_tokens[i])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 100000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)  # Count total unique tokens\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))  #Select top 10000 most occuring token\n",
    "    id2token = list(vocab)   #List of most common 10000 token. Entry at certain index is token\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab))))    #Maps token to index\n",
    "    id2token = ['<pad>', '<unk>'] + id2token  #Pad and unknown added\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 23873 ; token hilarious.<br /><br\n",
      "Token hilarious.<br /><br; token id 23873\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "#Bag of words takes token in each sentence in the model and embeds it in the continuous vector spce\n",
    "#Embedding is a simple table lookup. 10000 X embedding size matrix\n",
    "# We access vector for that index\n",
    "#We average those vectors and continuous representations together and pass to linear\n",
    "#function.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding. Use atleast 100 200 300 400 etc\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            Takes input. Embeds it. And then averages it. The length is used for\n",
    "            konwing actual length of sentence is padding is always used.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 80.98\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 84.86\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 85.36\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 84.86\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 84.04\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 85.78\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 86.04\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 84.66\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 84.88\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 84.04\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 83.24\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 82.92\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 84.14\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 84.22\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 83.98\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 84.18\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 84.3\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 83.64\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa1=pa\n",
    "pl1=pl\n",
    "epa1=epa\n",
    "epl1=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 73.58\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 81.02\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 81.74\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 80.7\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 79.84\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 81.64\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 80.98\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 81.34\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 79.5\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 79.24\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 80.84\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 80.96\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 79.52\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 80.78\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 79.46\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 75.94\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 79.1\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 78.72\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "pa2=pa\n",
    "pl2=pl\n",
    "epa2=epa\n",
    "epl2=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 78.96\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 79.2\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 79.64\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 79.6\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 79.54\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 79.66\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 79.58\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 79.72\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 79.78\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 79.84\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 79.8\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 79.86\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 79.82\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 80.16\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 80.22\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 80.18\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 80.14\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 80.16\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "pa3=pa\n",
    "pl3=pl\n",
    "epa3=epa\n",
    "epl3=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/625], Validation Acc: 76.78\n",
      "Epoch: [1/3], Step: [201/625], Validation Acc: 80.92\n",
      "Epoch: [1/3], Step: [301/625], Validation Acc: 81.98\n",
      "Epoch: [1/3], Step: [401/625], Validation Acc: 80.58\n",
      "Epoch: [1/3], Step: [501/625], Validation Acc: 76.94\n",
      "Epoch: [1/3], Step: [601/625], Validation Acc: 80.92\n",
      "Epoch: [2/3], Step: [101/625], Validation Acc: 75.5\n",
      "Epoch: [2/3], Step: [201/625], Validation Acc: 78.72\n",
      "Epoch: [2/3], Step: [301/625], Validation Acc: 81.38\n",
      "Epoch: [2/3], Step: [401/625], Validation Acc: 81.4\n",
      "Epoch: [2/3], Step: [501/625], Validation Acc: 81.42\n",
      "Epoch: [2/3], Step: [601/625], Validation Acc: 79.76\n",
      "Epoch: [3/3], Step: [101/625], Validation Acc: 80.86\n",
      "Epoch: [3/3], Step: [201/625], Validation Acc: 81.86\n",
      "Epoch: [3/3], Step: [301/625], Validation Acc: 80.36\n",
      "Epoch: [3/3], Step: [401/625], Validation Acc: 77.66\n",
      "Epoch: [3/3], Step: [501/625], Validation Acc: 82.04\n",
      "Epoch: [3/3], Step: [601/625], Validation Acc: 80.72\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "num_epochs = 3 # number epoch to train\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    #Forward propogates through the model.takes softmax to compute probabilities.\n",
    "    #Takes index corresponding to most likely label.\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        #counts how many are correct\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n",
    "pa4=pa\n",
    "pl4=pl\n",
    "epa4=epa\n",
    "epl4=epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx=[1,2,3]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epa1,label='0.01')\n",
    "plt.plot(xx,epa2,label='0.1')\n",
    "plt.plot(xx,epa3,label='0.001')\n",
    "plt.plot(xx,epa4,label='1')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx=[1,2,3]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(xx,epl1,label='0.01')\n",
    "plt.plot(xx,epl2,label='0.1')\n",
    "plt.plot(xx,epl3,label='0.001')\n",
    "plt.plot(xx,epl4,label='1')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
